---
title: |
  | \LARGE \textsf The Selection of a Model for Airlines Customer Satisfaction
  | \vspace{4ex}
author: "Joanna Krężel, Anna Matysek, Piotr Mikler, Adam Szczerba"
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "The project aims to analyze the data about satisfaction of Investico Airlines passengers. Having customer-granular observations about the cruise and a reported satisfaction level for particular aspects of the flight we try to fit models which predict whether they are satisfied with the service or not. The binary classification models used during the project are {} {} {}, out of which our recommendation is {} based on {}. This document describes the process we undertook and presents the results of data preprocessing, model selection and model validation."
documentclass: article 
classoption:
  - 12pt
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
header-includes:
    \usepackage{titling}
    \pretitle{\begin{center}
    \vspace{-7ex}
    \includegraphics[height=40mm]{img/logo3AGH.png}\\
    \vspace{5ex}
    {\large \bf \textsf{AGH UNIVERITY OF SCIENCE AND TECHNOLOGY}}\\
    {\large \bf \textsf{FACULTY OF APPLIED MATHEMATICS}}\\
    \vspace{10ex}
    }
---

<!--
Alternative, use the mgrwms template:
https://code.google.com/archive/p/mgrwms/downloads
http://zasoby.open.agh.edu.pl/~12sjkaminski/indexba7d.html?q=pl/content/klasy-agh
-->


\newpage

\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(forcats)
library(InformationValue)
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
library(ROCR)
library(modelr)
library(purrr)
library(httr)
library(caret)
library(woeBinning)

# SETUP
# Some config variables
bDownloadFromKaggle <- FALSE  # fetch data from Kaggle when compiling .Rmd
bRunComputationallyHeavyStuff <- FALSE # set ```{r eval=bRunComputationallyHeavyStuff}```
                                       # to control execution of a problematic cell from                                          # here

apiCredentials <- list(username = "piotrmikler",
                       key = "8453a8073400c47671d87e6a1b0fe7da")  # leave as-is
setwd("C:/Users/Joann/Desktop/Airlines")
```

\newpage

# Introduction

_Tbd..._

# Data

```{r}
fetch_from_kaggle <- function(sOutputPath, apiCredentials){
  #' Download the project dataset from Kaggle API
  #' 
  #' @param sOutputPath path-like string, location where to save data
  #'
  #' @param apiCredentials Kaggle API credentials, list of username and key

  sZipFile <- file.path(sOutputPath, 'Dataset.zip')
  apiResponse <- httr::GET("https://www.kaggle.com/api/v1/datasets/download/sjleshrac/airlines-customer-satisfaction",
                           httr::authenticate(apiCredentials$username, 
                                              apiCredentials$key, 
                                              type="basic"))
  # Download and unzip
  download.file(apiResponse$url, 
                destfile = sZipFile, 
                mode="wb")
  unzip(zipfile = sZipFile, 
        file = 'Invistico_Airline.csv',
        exdir = file.path(sOutputPath, "Data"))
  unlink(sZipFile)
}

if (bDownloadFromKaggle){
  fetch_from_kaggle(sOutputPath = getwd(),
                    apiCredentials = apiCredentials)
}

sFileName <- file.path(getwd(), '/Data/Invistico_Airline.csv')

AirlinesRaw <- read_csv(sFileName,
                        col_types = "fffdffdffffffffffffffdd")
```

The data is downloaded from [www.kaggle.com](https://www.kaggle.com/sjleshrac/airlines-customer-satisfaction?fbclid=IwAR1azFXqBkqKfah7ZTJ16CcR6S-zYpdQMJ7z7hlXcrEtzcJgh6nlN_4Bu8Y) and delivered by an airline organization. The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values. Also the airlines need to know on which aspect of the services offered by them have to be emphasized more to generate more satisfied customers. The data consists of 129880 rows and 23 columns.

Below we list all column names with explanations of the variables' meaning.
For categorical variables describing satisfaction level, $0$ means _Not Available_ and reflects situation in which the passenger did not provide a rating.

Feature | Description | Values
--------|-------------|-------
Satisfaction | Airline satisfaction level | satisfied/dissatisfied
Gender | Gender of the passenger | male/female
Customer type | The customer type | loyal / disloyal customer
Age | The age of a passenger | [7; 85] years
Type of travel | Purpose of the flight | personal / business travel
Class | Travel class in the plane | business / eco / eco plus
Flight distance | The flight distance of the journey | [50; 6951] miles
Seat comfort | Satisfaction level of seat comfort | {0-5}
Departure/arrival | Satisfaction level of departure/arrival time | {0-5}
Food and drink | Satisfaction level of food and drink | {0-5}
Gate location | Satisfaction level of gate location | {0-5}
Inflight WiFi service | Satisfaction level of the inflight wifi service | {0-5}
Inflight entertainment | Satisfaction level of inflight entertainment | {0-5}
Online support | Satisfaction level of online support | {0-5}
Ease of online booking | Satisfaction level of online booking | {0-5}
On-board services | Satisfaction level of on-board service | {0-5}
Leg room service | Satisfaction level of leg room service | {0-5}
Baggage handling | Satisfaction level of baggage handling | {0-5}
Checkin service |Satisfaction level of check-in service | {0-5}
Cleanliness | Satisfaction level of cleanliness | {0-5}
Online boarding | Satisfaction level of online boarding | {0-5}
Departure delay in minutes | Delay upon departure | [0; 1592] minutes
Arrival delay in minutes | Delay upon arrival | [0; 1584] minutes


## Exploratory Data Analysis
Before data wrangling and trying to fit some models, it is great to get an overview of our data set and verify whether it makes sense. We start from summarizing our to see basic statistical analysis. For each quantitive variable we get the minimum, maximum, mean, median and the inter quartile range information. However, for each categorical variable we get the number of observations in each category. 
```{r,echo=TRUE}
summary(AirlinesRaw)
```
\\
In this project we will predict the satisfaction level of a customer in function of the other variables based on past data. All results seems to be reasonable. It is noteworthy that the numbers of satisfied and dissatisfied customers are similar, which means that the data is valuable for future predictions. Moreover, we have almost the same number of males and females, and of business and eco class passengers. However, we have significantly more opinions from loyal customers than from disloyal ones. This may cause some unreliability in data since loyal customers might give greater notes because of some discounts for flights etc. We can also look at each variable connected with customers' notes and check which aspects are considered better and which worse. What we also notice is the fact that there are missing values in some columns. Nevertheless, we will take care about it later.\\

Additionally, we plot some histograms to view the distribution of quatitive variables.\\
```{r,echo=TRUE}
attach(AirlinesRaw)
h4<-histogram(AirlinesRaw$Age,AirlinesRaw,col='blue',type='count')
h7<-histogram(AirlinesRaw$`Flight Distance`,AirlinesRaw,col='blue',type='count')
h22<-histogram(AirlinesRaw$`Departure Delay in Minutes`,AirlinesRaw,col='blue',type='count')
h23<-histogram(AirlinesRaw$`Arrival Delay in Minutes`,AirlinesRaw,col='blue',type='count')
grid.arrange(h4,h7, h22, h23,ncol=2)
```
We see that:\\
• departure and arrivals delays are mostly small,\\
• the most common flight distance is around 2000 kilometers,\\
• the age differs from few years to more than 80, but the biggest number of customers is between 20 and 60.\\

Now, when we have an initial overview of our data set, we can move to preprocessing. 

## Data Preprocessing

As both academia and business point out, the data-related operations typically constitute about $80\%$ of the whole effort of a modeling pipeline. The performance of any model is heavily driven by the quality of it's inputs. It can be easily proven in a simple trial by combat that even a suboptimal model running on high quality data can oftentimes bring a sophisticated one with poor inputs to it's knees. For that reason it is of utmost importance to pay extra care and attention to the data which is fed to the decision making models.

During data preprocessing step we will gain insight about data statistics and information it conveys. First, we'll deal with NAs and outliers. Then we will refactor the features with variable binning and select a subset of features which are likely to display predictive power for our problem. In the end we will encode the variables and prepare a train-test split for model development.

```{r}
# Encoding Categorical Binary Features
sCategorialColnames <- c('satisfaction', 'Gender', 'Customer Type', 'Type of Travel')

Airlines <-
  AirlinesRaw %>% # Encode as 1 or 0 (Yes or No), store under new column names
  mutate(IsSatisfied = as_factor(ifelse(satisfaction == "satisfied", 1, 0)),
         IsFemale = as_factor(ifelse(Gender == "Female", 1, 0)),
         IsLoyal = as_factor(ifelse(`Customer Type` == "Loyal Customer", 1, 0)),
         IsPersonalTravel = as_factor(ifelse(`Type of Travel`== "Personal Travel", 1, 0))) %>%
  select(-all_of(sCategorialColnames)) %>% # drop old columns
  rename("FlightDistance" = "Flight Distance", # rename long column names
         "SeatNote" = "Seat comfort", 
         "ScheduleNote" = "Departure/Arrival time convenient",
         "FoodNote" = "Food and drink",
         "GateNote" = "Gate location",
         "WifiNote" = "Inflight wifi service",
         "EntertainmentNote" = "Inflight entertainment",
         "eSupportNote" = "Online support",
         "eBookingNote" = "Ease of Online booking",
         "ServiceNote" = "On-board service",
         "LegRoomNote" = "Leg room service",
         "BaggageNote" = "Baggage handling",
         "CheckInNote" = "Checkin service",
         "CleanNote" = "Cleanliness",
         "eBoardingNote" = "Online boarding",
         "DepartureDelay" ="Departure Delay in Minutes",
         "ArrivalDelay" = "Arrival Delay in Minutes") %>%
  mutate_at(vars(matches("Note")),  # in all "Note" features
            ~ replace(., which(.==0), NA)) %>%  # replace 0s with NAs
  mutate_at(vars(matches("Note")),  # in all "Note" features
            droplevels)  # drop empty factor levels (0s are no longer present)
```


### Missing data treatment


```{r NA_structure}
# Summarize NAs

NA_structure <-
  Airlines %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>%
  mutate(value = is.na(value)) %>%
  group_by(Feature) %>%
  summarise(NA_count = sum(value),
            pct_of_data = paste0(round(100*sum(value)/length(value), 2), "%")) %>%
  filter(NA_count > 0) %>%
  arrange(desc(NA_count))

knitr::kable(NA_structure,
             caption= "NA breakdown per feature. NAs span a small portion of data \\label{tab:NA_structure}")
```

After examining the data it seems we don't have any critical issue related to missing values. *NAs* are present in $12$ variables, but they constitute a minuscule portion of a very large dataset (see fig. \ref{tab:NA_structure}). We considered employing an imputation strategy based on median, but given that NAs constitute roughly `r round(mean(Airlines %>% is.na() %>% rowSums() > 0), 2)` of all observations, even if we drop them we would still have `r (Airlines %>% drop_na() %>% dim())[1]` observations left to work with. Based on that we decided not to introduce imputed values to the dataset, but rather work with pure data. 

```{r}
Airlines <- drop_na(Airlines)
```


### Feature Engineering

The more is not always the better. Feature engineering is a pre-modeling stage which serves identifying features which are significant and filtering out the ones that are not. *Filtering feature selection methods* allow one to discard redundant features in a model independent way. By reducing the number of variables or discretizing them we simplify the model and increase it's interpretability. It is also a step which tackles multicollinearity (high linear codependency of explanatory variables) which kills stability and predictive power of some models.

Across the following sections we are going to introduce a few additional modifications of variables to the dataset and see if it makes sense to keep them. Since we have a big share of $0s$ in column `DepartureDelay`, we wanted to check if it makes sense to include a binary variable $IsDelayed$ as a predictor. We are also going to try discretizing `Age`, `DepartureDelay` and `FlightDistance` continuous variables into bins based on Weight of Evidence metric and evaluate their predictive power.

#### Continuous Features

We have four continuous variables in our dataset: `Age`, `DepartureDelay`, `ArrivalDelay` and `FlightDistance`. We start the analysis by analyzing their codependence structure.
We note a high linear relationship between `ArrivalDelay` and `DepartureDelay`, visible both in the correlation matrix  (fig. \ref{fig:Multicollinearity}) and on figure \ref{fig:Delays_Collinearity}. We can safely drop `ArrivalDelay`, since it doesn't introduce new information and additionally contaminates the dataset with NAs.
```{r Multicollinearity, echo=FALSE, fig.width=4, fig.height=4, fig.cap="\\label{fig:Multicollinearity} Pearson correlation matrix only detects strong collinearity in Delays."}
multicorr <- 
  Airlines %>% 
  select(all_of(c('Age', 'DepartureDelay', 'ArrivalDelay', 'FlightDistance'))) %>%
  drop_na() %>%
  cor() %>%
  ggcorrplot(lab = T)

multicorr
```


```{r Delays_Collinearity, echo=FALSE, fig.width=3, fig.height=3, fig.cap="\\label{fig:Delays_Collinearity} Strong linear relationship between departure delay and arrival delay allows to drop one of them from the dataset."}
c <- cor(x = drop_na(Airlines)$DepartureDelay, y = drop_na(Airlines)$ArrivalDelay)
Airlines %>% drop_na() %>% 
  ggplot(aes(x = DepartureDelay, y = ArrivalDelay)) +
  geom_point() + geom_smooth()

Airlines$ArrivalDelay <- NULL
```

Next, we're going to examine the loess estimator of satisfaction as a function of the remaining continuous variables to see if any of them looks flat enough to raise suspicion regarding it's utility. Flatness of loess implies that average satisfaction does not change in the explanatory variable, hence the explanatory variable doesn't convey much information. 
In our case, this is not visible on the figure \ref{fig:loess}, so we cannot discard any feature based on that.  Note, the plot has been generated on a randomized data sub-sample for computational complexity reduction. We made sure however to take a sample large enough, so that the standard errors are tamed, and to ensure the relationship shape is stable regardless of the random seed chosen.

```{r, fig.width=5, fig.height=8, fig.caption = "Loess estimator for IsSatisfied as function of continuous features from the dataset.\\label{fig:loess}"}

set.seed(127)
Airlines_numeric <- 
  Airlines %>% 
  mutate_all(as.character) %>%
  mutate_all(as.numeric) %>%
  slice_sample(n = 5000)

p1 <- qplot(data = Airlines_numeric, Age, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("Age")
p2 <- qplot(data = Airlines_numeric, FlightDistance, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("FlightDistance")
p3 <- qplot(data = Airlines_numeric, DepartureDelay, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("DepartureDelay")

grid.arrange(p1, p2, p3, ncol=1)
```

Now we are going to look at possible binnings of our features. We'll use `woeBinning::woe.binning` function which chooses the binning to maximize the information value of the feature. If the optimized binning will yield $IV < 0.1$, we will discard the variable. Otherwise we'll analyze the bins to ensure they are not over-optimized to an unreasonable degree. Based on loess plot shapes/regimes the expectation is to have not more than four bins for `Age` and `FlightDistance`, and a maximum of two bins for `DepartureDelay`.  

```{r, results='asis'}
cols_to_bin <- Airlines %>% select(where(is.double)) %>% colnames
cutpoints <- Airlines %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', '1.Rate', 'WOE', "IV")

for (i in 1:length(cols_to_bin)){
  woetable <- binnings[i]
  available_cols <- colnames(woetable[[1]])

  cat(names(woetable[1]))
  woetable[[1]] %>% 
    select(all_of(intersect(display_cols, available_cols))) %>%
    knitr::kable() %>%
    print()
}
```

From the WOE tables above we see that `DepartureDelay` is a variable of low predictive power, hence we won't use it in modelling. For the other variables, as the data binning chosen by the algorithm seems reasonable given the ex-ante expectations, we're going to keep them.

```{r}
Airlines_binned <- Airlines %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  select(-all_of(c("Age", "FlightDistance", "DepartureDelay"))) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  as_tibble()

levels(Airlines_binned$FlightDistance) <- c("L", "M", "H", NA)
levels(Airlines_binned$Age) <- c("u30", "30s", "40s50s", "60plus", NA)
```

#### Ordinal & Categorical Features

The main challenge of the data preparation in this dataset is the proper treatment of passenger notes.
Take for example the `SeatNote` feature which is a customer note describing their satisfaction level with the seating arrangement. One could ask himself the following questions:

* What did the passenger have in mind? Satisfaction with their seat location? Seat comfort? Possibility of choosing the seat?
* Does $`SeatNote`=3$ imply a negative attitude towards a service? Or it's a moderate 'OK'?
* Is the satisfaction *"difference"* between notes $3$ and $2$ the same as between notes $5$ and $4$?
* Is a note $`SeatNote`=5$ given $`Class`=`Eco`$ the same as $`SeatNote`=5$ given $`Class`=`Business`$?

The same point is valid for any note-type variable in the dataset. The issue boils down to the problem that there is **no universal "unit" of satisfaction**. It is just as non-trivial to measure it as to predict it - simply because everyone perceives satisfaction in a subjective way. Our problem has an additional layer of complexity since we don't have information how precisely the survey questions have been described to the customers - so even if we *did* have some carefully designed satisfaction unit, we cannot be sure if all respondents referred to the same aspects of service when filling out the survey.

We aim to overcome this problem, by binning notes into wider classes, depending on how well they explain and affect the overall satisfaction. Since we have a lot of 5-leveled `Note` factors, there's a strong suspicion that in such large set there must exist some adjacent levels such that overall satisfaction is invariant to displacements in that group of levels. In other words, we could collapse notes of $1, 2 \& 3$ to one group if they carried similar information. Hence we will again let `woe.binning` automatically select bins and then verify the result.

Since there are $12$ `Note` variables, we will only display one of the WOE tables as an example. However for all it has been verified that **adjacent** levels have been binned, so the binning is plausible, and the $IV$ of the newly binned features are above $0.1$.
```{r, results='asis'}
cols_to_bin <- Airlines_binned %>% select(c(ends_with('Note'))) %>% colnames
cutpoints <- Airlines_binned %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', 'WOE', "IV")

Airlines_binned <- Airlines_binned %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  as_tibble() %>%
  select(-all_of(cols_to_bin)) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  mutate_at(vars(matches("Note")), ~as_factor(ifelse(grepl("1", .), "L",
                                                 ifelse(grepl("5", .), "H",
                                                        "M"))))
binnings$`WOE Table for eBookingNote` %>% knitr::kable()
```

Next, we will check the information value for other categorical variables. They are binary, so binning has not been applied to them in the earlier step. The table \ref{tab:CategoricalFeatureSelection} presents features and their IV. We see that there are features with IV smaller than $0.1$ - we are going to drop those from the dataset. 

Lastly, a brief look at the spearman rank correlation matrix (fig. \ref{fig:spearman_cor}) shows that there are no highly correlated "note" features among ordinal variables in the dataset.

```{r CategoricalFeatureSelection, caption="Categorical features with Information Value smaller than 0.1\\label{tab:CategoricalFeatureSelection}"}
sCategoricalFeatures <- 
  Airlines_binned %>%
  colnames() %>% 
  setdiff(c("IsSatisfied"))

output <- data.frame(varName = character(0),
                     IV = numeric(0))  # create empty data frame

for (colname in sCategoricalFeatures) {
   wt <- WOETable(Airlines_binned[[colname]], 
                  Airlines_binned$IsSatisfied, 
                  valueOfGood = 1)
   
   tmp  <- data.frame(varName = colname, 
                      IV = sum(wt$IV))
   
   output <- rbind(output, tmp)
}

vars_to_drop <- 
  output %>% arrange(IV) %>% filter(IV<0.1) %>% 
  pull(varName) 

table_ <- output %>% arrange(IV) %>% filter(IV < 0.1)
knitr::kable(arrange(output, IV), 
             caption = "Information Value for all variables.\\label{tab:CategoricalFeatureSelection}")
```

```{r}
Airlines_binned <- Airlines_binned %>% select(-all_of(c(vars_to_drop)))
```

```{r CategoricalSpearman,  echo=FALSE, fig.width=7,fig.height=6,fig.cap="\\label{fig:spearman_cor} Spearman correlation shows no significant colinear relationships in ordinal variables"}
spearman_cor <-
  Airlines_binned %>%
  select(ends_with("Note")) %>%
  mutate_all(~as_factor(ifelse(. == "L", 0,
                               ifelse(. == "M", 1, 2)))) %>%
  mutate_all(as.numeric) %>%
  cor(method='spearman')

spearman_cor %>%
  ggcorrplot(lab=T)
```

### Feature Encoding

Some machine learning algorithms require numerical data, so we considered **ordinal encoding** and **dummy encoding** to transform our data.

We ran the following thought experiment to determine which encoding to employ. Say we use ordinal encoding and assign numbers to each factor level. We could encode `Class` this way and assign a mapping like: {'Eco': 1, 'EcoPlus': 2, 'Business': 3}. This type of representation ensures the quality of the service is properly represented in the numeric data, but the question is whether this translates the same to the overall satisfaction? Yes, the business class is clearly more comfortable to travel in, but the *expectations* (the baseline) of business-class passengers will also be quite higher than the expectations of say, passengers in the economic class.

We chose to employ dummy encoding to encode `Class` - to avoid making assumptions about baseline satisfaction criteria across different passenger classes. For `Note` features however this problem is non-existent, since a higher note should correspond to higher satisfaction for any rational passenger. Here to avoid inflating the dataset with additional $4\cdot14 - 14 = 42$ sparse binary columns we will stick to the original ordinal encoding. This choice nonetheless should be revisited and controlled once we reach the stage of model choice and model fitting.

The resulting, encoded dataframe looks the following way:
```{r}
# Encode Class feature
Airlines_binned <- Airlines_binned %>% 
  mutate(Class = as_factor(str_replace(Class, " ", ""))) # Eco Plus -> EcoPlus

vars_to_encode <-
  Airlines_binned %>% 
  select(c(ends_with('Note'), 'Age', 'FlightDistance', 'Class')) %>%
  colnames()

formula_string <- paste0("~ ", paste(vars_to_encode, collapse=" + "))

AirlinesEncoded_binned <- 
  dummyVars(as.formula(formula_string), data = Airlines_binned, 
            fullRank = TRUE, drop2nd = TRUE) %>%  # ensure 1 is dropped
  predict(newdata = Airlines_binned) %>%  # Execute the split
  as_tibble() %>%
  bind_cols(Airlines_binned) %>% # Join dummy features with the original dataset
  select(-all_of(vars_to_encode)) %>%  # drop old 'Class' feature
  mutate_all(as_factor) # ensure all are factors

AirlinesEncoded_binned %>% glimpse()
```  



# A train-test split of data
```{r}
# First we build the random indexes:
N <- nrow(Airlines_binned)
train_index <- sample(1:N, 0.8 * N)
test_index  <- setdiff(1:N, train_index)

# Set "L" as the base level
Airlines_binned <- 
  Airlines_binned %>% 
  mutate_at(vars(ends_with("Note")), ~relevel(. , "L")) %>%
  mutate_at(vars(ends_with("FlightDistance")), ~relevel(. , "L"))

# Then we build the training and test data set:
Airlines_binned_train <- Airlines_binned[train_index, ]
Airlines_binned_test  <- Airlines_binned[test_index, ]
AirlinesEncoded_binned_train <- AirlinesEncoded_binned[train_index, ]
AirlinesEncoded_binned_test  <- AirlinesEncoded_binned[test_index, ]
```

# Baseline Model

## Fitting and performance
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Validation

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# The Challenger Models

## Random Forest

### Fitting and performance
We will try out Random Forest method. Firstly, we consider model including all of the variables.
```{r}
library(randomForest)
#Define the formula
frm<-IsSatisfied~.
#Fit the model
set.seed(12428)
RanForest1<-randomForest(frm,Airlines_binned_train,importance=TRUE)
#Investigate the model
RanForest1
```
After running Random Forest algorithm on basic model we can be quite pleased with the results. Only 6.18% of the observations were misqualified thus we obtained model with 93.82% accuracy.\

We denote number of variables tried at each split as \textbf{mtry}. This parameter allows us to choose optimal number of generated decision trees. By optimal we mean such \textbf{mtry} that value of OOB estimate of error rate is the lowest.
```{r}
set.seed(12428)
mtry<-tuneRF(Airlines_binned_train[-2],Airlines_binned_train$IsSatisfied,ntreeTry=500,stepFactor=1.5,improve=0.01,trace=TRUE,plot=TRUE)
```
As we see, the value chosen by deafult is the best one.\

Random Forest enables us to determine how important each variable is in the model, two criteria are used:\
•MeanDecreaseAccuracy - expresses the accuracy lost by leaving particular variable out of the training set. \
•MeanDecreaseGini - measures node impurity at each split, highest purity means that each node contains only elements of a single class.
```{r}
varImpPlot(RanForest1)
```
In terms of accuracy of the model, eBoardingNote, WifiNote and ServiceNote seem to be the least important. The most influential are SeatNote, CheckInNote and eSupportNote. The accuracy of the model could drop significantly if we left them out. As for Gini coefficient, the most important are EntertainmentNote, SeatNote and eBookingNote and the least ServiceNote, WifiNote and eBoardingNote.\

SeatNote is placed high on both plots, so we may assume that this variable has strong influence on satisfaction level. The same applies to eSupportNote and EntertainmentNote. While thinking of clients' satisfaction, the airline should definitely focus on providing them with comfortable seats and proper online customer support. Moreover, making sure that passengers are entertained throughout the flight (e.g. movies, headphones with music) will definitely help them look back at the flight with nothing but enjoyment.\

While thinking of the least important variables we have one immediate candidate - WifiNote - it seems like availability and quality of WiFi service does not matter that much to the customers.\

Now, with the help of the ROCurve, we will check the performance of this model.
 
```{r}
# function to get AUC
fAUC_RF <- function(model, data, ...) {
  y <- all.vars(formula(model))[1]
  pred1 <- as.numeric(predict(model, newdata = data, ...))
  pred <- ROCR::prediction(pred1, data[[y]])
  perf <- performance(pred, "auc")
  AUC <- attr(perf, "y.values")[[1]]
  AUC
}
AUC_RanForest1_train <- fAUC_RF(RanForest1, data = Airlines_binned_train, type = "response")
AUC_RanForest1_test <- fAUC_RF(RanForest1, data = Airlines_binned_test, type = "response")
pred1_RF <- prediction(as.numeric(predict(RanForest1, type = "prob")[ ,2]), Airlines_binned_train$IsSatisfied)

# Visualize the ROC curve:
plot(performance(pred1_RF, "tpr", "fpr"), col="blue", lwd = 3)
AUC1_RF <- attr(performance(pred1_RF, "auc"), "y.values")[[1]]
perf1_RF <- performance(pred1_RF, "tpr", "fpr")
ks1_RF <- max(attr(perf1_RF,'y.values')[[1]] - attr(perf1_RF,'x.values')[[1]])
predSc1_RF <- as.numeric(predict(RanForest1, type = "prob")[ ,2])
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predSc1_RF)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predSc1_RF)
```
The model has an AUC of `r AUC1_RF` on training data and `r AUC_RanForest1_test` on testing data. As the difference between them is small we may presume that this model would be a good choice, plots also look satisfying.\

Just to make sure that we don't miss out on any better model, we decided to check some of the options for improvement but did not succeed. Excluding any variables resulted in making accuracy worse. Adding interactions didn't change accuracy or performance of the model at all. Our final choice is the basic model. \

We can now approach validation part.

### Validation
We will validate the model using the cross validation method and opt for the Monte Carlo Cross Validation. We will draw 100 times a training data-set containing 70% of observations and then study the AUC on the testing data-set.
```{r}
pctTrain <- 0.7
set.seed(1201984)
nRuns = 10
```

```{r fig.cap='The results of the cross validation for the random forest.\\label{fig:MCxvalrf1}'}
cv_mc_RF <- crossv_mc(Airlines_binned_train, n = nRuns, test = 1 - pctTrain)
mods_RF<-map(cv_mc_RF$train,~RanForest1)
AUCs_train_RF <- numeric(0)
AUCs_test_RF <- numeric(0)
for(k in 1:nRuns) {
AUCs_test_RF[k] <- fAUC_RF(mods_RF[[k]], as.data.frame(cv_mc_RF$test[[k]]))
AUCs_train_RF[k] <- fAUC_RF(mods_RF[[k]], as.data.frame(cv_mc_RF$train[[k]]))
}
allAUCs_RF <- rbind(tibble(model = "train data", AUC = AUCs_train_RF),
tibble(model = "test data", AUC = AUCs_test_RF))
p1 <- ggplot(allAUCs_RF, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5)
p2 <- ggplot(allAUCs_RF, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
```
Observed values for the AUC of the test data have the median equal to `r median(AUCs_test_RF)`, the average equal to `r mean(AUCs_test_RF)` and standard deviation equal to `r sd(AUCs_test_RF)`. Given the plots and parameters we are content with the results, AUCs of training and test sets keep in line with each other. The model does not over-fit so we find it valid.


## Logistic Regression

### Fitting and performance
The next model that we will study is a logistic regression. We start from the most basic model which takes all the variables. 
```{r}
# Define the formula:
frm <- IsSatisfied ~.
# Fit the model:
logreg1 <- glm(formula = frm, data = Airlines_binned_train, family = "binomial")
# Investigate the model:
summary(logreg1)

```
We notice that according to this model:\\
• business class passengers have a much higher satisfaction level than eco plus class passengers, who in their turn have a higher satisfaction level than eco class ones,\\
• satisfaction level is higher for females and loyal customers,\\
• passengers at the age of 40s and 50s have a higher satisfaction level than the others,\\
• satisfaction level is lower for long and medium flight distances than for the short ones,\\
• the higher entertainment, seat, eBooking, eSupport, service, leg room, clean, baggage and check-in notes, the higher the satisfaction level.\\

What we find weird is that taking into account wifi, food and eBoarding the satisfaction level is higher for medium notes than for the high ones. This may be a consequence of the fact that something went wrong and the airlines wanted to compensate for this for example with food. Another reason could be connected with the flight distance. Wifi and more food are usually available during long flights. However, the longer the flight, the more things can go wrong and the more people are tired, uncomfortable etc. Nevertheless, we don't have enough background information to make such conclusions. We decide to leave these variables in our model for now and check its performance, but later we will try do improve it.
```{r}
library(ROCR)
# function to get AUC
fAUC <- function(model, data, ...) {
y <- all.vars(formula(model))[1]
pred1 <- predict(model, newdata = data, ...)
pred <- ROCR::prediction(pred1, data[[y]])
perf <- performance(pred, "auc")
AUC <- attr(perf, "y.values")[[1]]
AUC
}
AUC_logreg1_train <- fAUC(logreg1, data = Airlines_binned_train, type = "response")
AUC_logreg1_test <- fAUC(logreg1, data = Airlines_binned_test, type = "response")
pred1 <- prediction(predict(logreg1, type = "response"), Airlines_binned_train$IsSatisfied)

# Visualize the ROC curve:
plot(performance(pred1, "tpr", "fpr"), col="blue", lwd = 3)
AUC1 <- attr(performance(pred1, "auc"), "y.values")[[1]]
perf1 <- performance(pred1, "tpr", "fpr")
ks1 <- max(attr(perf1,'y.values')[[1]] - attr(perf1,'x.values')[[1]])
predScores1 <- predict(logreg1, type = "response")
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores1)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores1)
```
The AUC on the training data is `r AUC1` and on the testing data `r AUC_logreg1_test`. The difference is small. KS plot and the ROC curve also looks great. This model is a good contender. However, we decide to try adding some interactions between variables as some of them seem natural. After many attempts we opt for including the interactions between gender&loyalty, gender&class, gender&age and class&flight distance. We tried also for example class&food or class&service, but they didn't improve our model. What is more, we decided not to include WifiNote as after adding interactions it was the least significant and the relationship was weird. Thus we get the final logistic regression model, which is summarized below.\\
```{r}
frm2 <- IsSatisfied ~.+IsFemale*IsLoyal+IsFemale*Age+IsFemale*Class+Class*FlightDistance-WifiNote
# Fit the model:
logreg2 <- glm(formula = frm2, data = Airlines_binned_train, family = "binomial")
# Investigate the model:
summary(logreg2)
```

We notice that according to our model:\\
• business class passengers have a much higher satisfaction level than eco plus class passengers, who in their turn have a higher satisfaction level than eco class ones,\\
• satisfaction level is higher for females and loyal customers,\\
• passengers at the age of 40s and 50s have a higher satisfaction level than the others,\\
• the higher entertainment, seat, eBooking, eSupport, service, leg room, clean, baggage and check-in notes, the higher the satisfaction level,\\
• generally, the longer the flight distance, the less satisfied people are. However, this is not the case in business cass, where people are more satisfied on longer distances,\\
• females travelling business and eco plus class are less satisfied than the ones from eco class and females over 60 are more satisfied than the younger ones.


Now we will check the performance of the model using few criteria.
```{r rocr, fig.cap=c("The ROC (receiver operating curve) for our model", "The lift of the model (bottom): the cumulative percentage of responders (ones) captured by the model")}
AUC_logreg2_train <- fAUC(logreg2, data = Airlines_binned_train, type = "response")
AUC_logreg2_test <- fAUC(logreg2, data = Airlines_binned_test, type = "response")
pred2 <- prediction(predict(logreg2, type = "response"), Airlines_binned_train$IsSatisfied)

# Visualize the ROC curve:
plot(performance(pred2, "tpr", "fpr"), col="blue", lwd = 3)
AUC2 <- attr(performance(pred2, "auc"), "y.values")[[1]]
perf2 <- performance(pred2, "tpr", "fpr")
ks2 <- max(attr(perf2,'y.values')[[1]] - attr(perf2,'x.values')[[1]])
predScores2 <- predict(logreg2, type = "response")
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores2)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores2)
```

The logistic regression model has an AUC of `r AUC2` on the training data and `r AUC_logreg2_test`on the testing data. The difference is very small. The KS is `r ks2`. Moreover, we can see (on the ROC curve graph) that our classifier is not far from being perfect. Therefore, we will move forward to the validation part. 

### Validation
To validate the model we will use the cross validation method and opt for the Monte Carlo Cross Validation.
```{r}
pctTrain <- 0.7
set.seed(18901229)
nRuns = 10
```
We use the Monte Carlo Cross Validation with a test data-set that spans 30% of our observations and 70% in the training data-set. We will draw 100 times a training data-set of 0.7 and study the AUC on the testing data-set.
```{r fig.cap='The histogram for the AUC of the ramdomised data-sets with the Monte Carlo cross vailidation for the first logistic regression model.\\label{fig:MCxval1}'}
cv_mc <- crossv_mc(Airlines_binned_train, n = nRuns, test = 1 - pctTrain)
mods <- map(cv_mc$train, ~ glm(frm2, data = ., family = "binomial"))
#AUCs <- map2_dbl(mods, cv_mc$test, fAUC)
RMSE <- map2_dbl(mods, cv_mc$test, rmse)
AUCs_train <- numeric(0)
AUCs_test <- numeric(0)
for(k in 1:nRuns) {
AUCs_test[k] <- fAUC(mods[[k]], as.data.frame(cv_mc$test[[k]]))
AUCs_train[k] <- fAUC(mods[[k]], as.data.frame(cv_mc$train[[k]]))
}
allAUCs <- rbind(tibble(model = "train data", AUC = AUCs_train),
tibble(model = "test data", AUC = AUCs_test))
p1 <- ggplot(allAUCs, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5)
p2 <- ggplot(allAUCs, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
median(AUCs_test)
mean(AUCs_test)
sd(AUCs_test)

```
We are satisfied with the performance of our model. The median of the observed values for the AUC of the test data is `r median(AUCs_test)`, the average is `r mean(AUCs_test)` with a standard deviation of `r sd(AUCs_test)`. We find these results great.

## State-of-the-art: Neural Network model

### Fitting and performance
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Validation
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Bibliography
De Brouwer, Philippe J.S. 2020. The Big r-Book: From Data Science to Learning Machines and Big Data. New York: John Wiley & Sons, Ltd.