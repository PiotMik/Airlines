---
title: |
  | \LARGE \textsf The Selection of a Model for Airlines Customer Satisfaction
  | \vspace{4ex}
author: "Joanna Krężel, Anna Matysek, Piotr Mikler, Adam Szczerba"
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "The project aims to analyze the data about satisfaction of Investico Airlines passengers. Having customer-granular observations about the cruise and a reported satisfaction level for particular aspects of the flight we try to fit models which predict whether they are satisfied with the service or not. The binary classification models used during the project are {} {} {}, out of which our recommendation is {} based on {}. This document describes the process we undertook and presents the results of data preprocessing, model selection and model validation."
documentclass: article 
classoption:
  - 12pt
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
header-includes:
    \usepackage{titling}
    \pretitle{\begin{center}
    \vspace{-7ex}
    \includegraphics[height=40mm]{img/logo3AGH.png}\\
    \vspace{5ex}
    {\large \bf \textsf{AGH UNIVERITY OF SCIENCE AND TECHNOLOGY}}\\
    {\large \bf \textsf{FACULTY OF APPLIED MATHEMATICS}}\\
    \vspace{10ex}
    }
---

<!--
Alternative, use the mgrwms template:
https://code.google.com/archive/p/mgrwms/downloads
http://zasoby.open.agh.edu.pl/~12sjkaminski/indexba7d.html?q=pl/content/klasy-agh
-->


\newpage

\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(forcats)
library(InformationValue)
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
library(ROCR)
library(modelr)
library(purrr)
library(httr)
library(caret)

# SETUP
# Some config variables
bDownloadFromKaggle <- FALSE  # fetch data from Kaggle when compiling .Rmd
apiCredentials <- list(username = "piotrmikler",
                       key = "8453a8073400c47671d87e6a1b0fe7da")  # leave as-is
setwd("D:/Classes/Mgr/Semestr_3/Quantitative_Analysis_for_Managerial_Decisions/Airlines") # change for your main directory
```

\newpage

# Introduction

_Tbd..._

# Data

```{r}
fetch_from_kaggle <- function(sOutputPath, apiCredentials){
  #' Download the project dataset from Kaggle API
  #' 
  #' @param sOutputPath path-like string, location where to save data
  #'
  #' @param apiCredentials Kaggle API credentials, list of username and key

  sZipFile <- file.path(sOutputPath, 'Dataset.zip')
  apiResponse <- httr::GET("https://www.kaggle.com/api/v1/datasets/download/sjleshrac/airlines-customer-satisfaction",
                           httr::authenticate(apiCredentials$username, 
                                              apiCredentials$key, 
                                              type="basic"))
  # Download and unzip
  download.file(apiResponse$url, 
                destfile = sZipFile, 
                mode="wb")
  unzip(zipfile = sZipFile, 
        file = 'Invistico_Airline.csv',
        exdir = file.path(sOutputPath, "Data"))
  unlink(sZipFile)
}

if (bDownloadFromKaggle){
  fetch_from_kaggle(sOutputPath = getwd(),
                    apiCredentials = apiCredentials)
}

sFileName <- file.path(getwd(), '/Data/Invistico_Airline.csv')

AirlinesRaw <- read_csv(sFileName,
                        col_types = "fffdffdffffffffffffffdd")
```

The data is downloaded from [www.kaggle.com](https://www.kaggle.com/sjleshrac/airlines-customer-satisfaction?fbclid=IwAR1azFXqBkqKfah7ZTJ16CcR6S-zYpdQMJ7z7hlXcrEtzcJgh6nlN_4Bu8Y) and delivered by an airline organization. The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values. Also the airlines need to know on which aspect of the services offered by them have to be emphasized more to generate more satisfied customers. The data consists of 129880 rows and 23 columns.

Below we list all column names with explanations of the variables' meaning.
For categorical variables describing satisfaction level, $0$ means _Not Available_ and reflects situation in which the passenger did not provide a rating.

Feature | Description | Values
--------|-------------|-------
Satisfaction | Airline satisfaction level | satisfied/dissatisfied
Gender | Gender of the passenger | male/female
Customer type | The customer type | loyal / disloyal customer
Age | The age of a passenger | [7; 85] years
Type of travel | Purpose of the flight | personal / business travel
Class | Travel class in the plane | business / eco / eco plus
Flight distance | The flight distance of the journey | [50; 6951] miles
Seat comfort | Satisfaction level of seat comfort | {0-5}
Departure/arrival | Satisfaction level of departure/arrival time | {0-5}
Food and drink | Satisfaction level of food and drink | {0-5}
Gate location | Satisfaction level of gate location | {0-5}
Inflight WiFi service | Satisfaction level of the inflight wifi service | {0-5}
Inflight entertainment | Satisfaction level of inflight entertainment | {0-5}
Online support | Satisfaction level of online support | {0-5}
Ease of online booking | Satisfaction level of online booking | {0-5}
On-board services | Satisfaction level of on-board service | {0-5}
Leg room service | Satisfaction level of leg room service | {0-5}
Baggage handling | Satisfaction level of baggage handling | {0-5}
Checkin service |Satisfaction level of check-in service | {0-5}
Cleanliness | Satisfaction level of cleanliness | {0-5}
Online boarding | Satisfaction level of online boarding | {0-5}
Departure delay in minutes | Delay upon departure | [0; 1592] minutes
Arrival delay in minutes | Delay upon arrival | [0; 1584] minutes

## Data Preprocessing

As both academia and business point out, the data-related operations typically constitute about $80\%$ of the whole effort of a modeling pipeline. The performance of any model is heavily driven by the quality of it's inputs. It can be easily proven in a simple trial by combat that even a suboptimal model running on high quality data can oftentimes bring a sophisticated one with poor inputs to it's knees. For that reason it is of utmost importance to pay extra care and attention to the data which is fed to the decision making models.

As the first step in our modeling pipeline we are going to look at the dataset to gain 
insight about it's statistics and information it conveys. We'll refactor the feature names to something more manageable and represent accordingly different data types present in the dataset. Lastly we will perform quality checks on the data, such as outlier detection and treatment of not-available values (*NAs*).

### Feature Encoding

Most machine learning algorithms require numerical inputs. Our data is mostly categorical and ordinal, hence we need to start with encoding those features.

#### Categorical features

The dataset contains some binary categorical information such as *Male/Female*, *Loyal/Disloyal Customer*, etc. We are going to employ binary encoding for those features, that is: map values to $1$ or $0$ and rename the factors to  `IsSatisfied`, `IsFemale`, `IsLoyal` for easier interpretation.

```{r}
# Encoding Categorical Binary Features
sCategorialColnames <- c('satisfaction', 'Gender', 'Customer Type', 'Type of Travel')

Airlines <-
  AirlinesRaw %>% # Encode as 1 or 0 (Yes or No), store under new column names
  mutate(IsSatisfied = as_factor(ifelse(satisfaction == "satisfied", 1, 0)),
         IsFemale = as_factor(ifelse(Gender == "Female", 1, 0)),
         IsLoyal = as_factor(ifelse(`Customer Type` == "Loyal Customer", 1, 0)),
         IsPersonalTravel = as_factor(ifelse(`Type of Travel`== "Personal Travel",1, 0))) %>%
  select(-all_of(sCategorialColnames)) %>% # drop old columns
  rename("FlightDistance" = "Flight Distance", # rename long column names
         "SeatNote" = "Seat comfort", 
         "ScheduleNote" = "Departure/Arrival time convenient",
         "FoodNote" = "Food and drink",
         "GateNote" = "Gate location",
         "WifiNote" = "Inflight wifi service",
         "EntertainmentNote" = "Inflight entertainment",
         "eSupportNote" = "Online support",
         "eBookingNote" = "Ease of Online booking",
         "ServiceNote" = "On-board service",
         "LegRoomNote" = "Leg room service",
         "BaggageNote" = "Baggage handling",
         "CheckInNote" = "Checkin service",
         "CleanNote" = "Cleanliness",
         "eBoardingNote" = "Online boarding",
         "DepartureDelay" ="Departure Delay in Minutes",
         "ArrivalDelay" = "Arrival Delay in Minutes") %>%
  mutate_at(vars(matches("Note")),  # in all "Note" features
            ~ replace(., which(.==0), NA)) %>%  # replace 0s with NAs
  mutate_at(vars(matches("Note")),  # in all "Note" features
            droplevels)  # drop empty factor levels (0s are no longer present)
```

#### Ordinal features

The main challenge of the data preparation in this dataset is the treatment of ordinal features.
Take for example the `SeatNote` feature which is a customer note describing their satisfaction level with the seating arrangement. One could ask himself the following questions:

* What did the passenger have in mind? Satisfaction with seat location? Seat comfort? Possibility of choosing the seat?
* Does $`SeatNote`=3$ imply a negative attitude towards a service? Or it's a moderate 'OK'?
* Is the satisfaction *"difference"* between notes $3$ and $2$ the same as between notes $5$ and $4$?
* Is a note $`SeatNote`=5$ given $`Class`=`Eco`$ the same as $`SeatNote`=5$ given $`Class`=`Business`$?

The point is valid for any note-type variable in the dataset. The issue boils down to the problem that there is **no universal "unit" of satisfaction**. It is just as non-trivial to measure it as to predict it - simply because everyone perceives it in a subjective way. Our problem has an additional layer of complexity since we don't have information how precisely the survey questions have been described to the customers - so even if we *did* have some carefully designed satisfaction unit, we cannot be sure if all respondents referred to the same aspects of service when filling out the survey.

Before discussing this further let's take a short detour to the options we have when dealing with ordinal variables for Machine Learning. Two most common approaches emerge: **Dummy encoding** and **Ordinal encoding** - both are valid, depending on what we're trying to achieve. 

We could use ordinal encoding and assign numbers to each vote. This is pretty much what we already have in our "note" features. We could encode `Class` this way and assign a mapping like: {'Eco': 1, 'EcoPlus': 2, 'Business': 3}. This type of representation ensures the quality of the service is properly represented in the numeric data, but the question is whether this translates the same to the overall satisfaction? Yes, the business class is clearly more comfortable to travel in, but the *expectations* (the baseline) of business-class passengers will also be quite higher than the expectations of say, passengers in the economic class. This may result in a counterintuitive drop in the satisfaction level, simply because the sub-populations across business classes will perceive the service differently. 

The second possibility we have for encoding ordinal variables is the *dummy encoding* which will split the feature `Class` into features: `Class.Eco`, `Class.EcoPlus` and `Class.Business` assigning ones and zeros in appropriate places. One of those features will be dropped to avoid perfect linear relationship (otherwise the sum of the new features would always be $1$), but we'll not lose information. We only need $n-1$ features to encode full information about a factor with $n$ possible levels.

We chose to employ dummy encoding to encode `Class` - to avoid making assumptions about baseline satisfaction criteria across different passenger classes. For `Note` features however this problem is non-existent, since a higher note should correspond to higher satisfaction for any rational passenger. Here to avoid inflating the dataset with additional $4*14 - 14 = 42$ sparse binary columns we will stick to the original ordinal encoding. This choice nonetheless should be revisited and controlled once we reach the stage of model choice and model fitting.

The resulting, encoded dataframe looks the following way:
```{r}
# Encode Class feature
Airlines <- Airlines %>% 
  mutate(Class = as_factor(str_replace(Class, " ", ""))) # Eco Plus -> EcoPlus

AirlinesEncoded <- 
  dummyVars(~Class, data = Airlines,  # Prepare a split into 3 dummy features
            fullRank = TRUE, drop2nd = TRUE) %>%  # ensure 1 is dropped
  predict(newdata = Airlines) %>%  # Execute the split
  as_tibble() %>%
  bind_cols(Airlines) %>% # Join dummy features with the original dataset
  select(-Class) %>%  # drop old 'Class' feature
  mutate_at(vars(matches("Class")), as_factor) # ensure all are factors

AirlinesEncoded %>% glimpse(width = 50)
```  

### Missing data treatment

In the dataset we have *NAs* for several features. We see proper *NAs* in the `ArrivalDelay` column, but there are also some "hidden" *NAs* represented by zeros, corresponding to a missing customer note. Let's tackle that issue in this short section. 

```{r NA_structure, echo=FALSE, fig.width=7,fig.height=6,fig.cap="\\label{fig:NA_structure} NA values constitute small percentage of the dataset"}
# Summarize NAs
AirlinesEncoded %>%
  select(c(ends_with("Note"), starts_with("Arrival"))) %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>%
  mutate(value = is.na(value)) %>%
  group_by(Feature) %>%
  summarise(NA_pct = mean(value)*100,
            nonNA_pct = 100*(1 - mean(value))) %>%
  filter(NA_pct > 0) %>%
  pivot_longer(cols=!ends_with('Feature'), names_to = "NA_stat") %>%
  ggplot(aes(x = "", y = value, fill=NA_stat)) + 
  geom_bar(stat='identity') + coord_polar("y", start=0) + facet_wrap(~Feature)
```

Generally speaking we don't have any critical issue related to missing values in our data. Yes, there are *NAs* present in $14$ variables, but they constitute a minuscule portion of a very large dataset (see fig. \ref{fig:NA_structure}).

Therefore we stand before three feasible choices:

* impute the missing values
* drop the rows from the dataset
* discard the feature from the dataset

Handling missingness of `ArrivalDelay` turns out to be very straightforward, due to very strong linear relationship with `DepartureDelay` (see fig. \ref{fig:Delays_Collinearity}). Looking at their scatterplot we see that we could easily impute missing values in `ArrivalDelay` by regressing it on `DepartureDelay`. It is also a very reasonable relationship, as intuitively the airplane departure delay should translate to delay in it's arrival roughly linearly. Given that the actual linear model beta is `r round(lm(ArrivalDelay~DepartureDelay, data = AirlinesEncoded)$coefficients['DepartureDelay'], 4)`, this imputation would be easy to justify and defend. 

```{r Delays_Collinearity, echo=FALSE, fig.width=7,fig.height=6,fig.cap="\\label{fig:Delays_Collinearity} Strong linear relationship between departure delay and arrival delay allows to use one for imputing missing values in the other."}
c <- cor(x = drop_na(Airlines)$DepartureDelay, y = drop_na(Airlines)$ArrivalDelay)
Airlines %>% drop_na() %>% 
  ggplot(aes(x = DepartureDelay, y = ArrivalDelay)) +
  geom_point() + geom_smooth() +
  ggtitle(paste0("Linearity of delays (corr: ", round(c, 4), ")"))
```

Therefore we could _technically_ easily impute the values by regressing `ArrivalDelay` on `DepartureDelay` - however given the high correlation of those variables (`r round(c, 4)` pearson correlation coefficient) one of them is bound to be dropped during multicollinearity analysis. For this reason we are not going to bother imputing the missing values, but will simply drop `ArrivalDelay` from the features at the stage of feature filtering.

Imputing the missing values in "note" variables would require much more effort though. We could take an impute-by-model approach, but that would require selecting, fitting and evaluating a multilabel response model (like multinomial or even ordinal logistic regression). We could alternatively impute by some selected data statistic, like the median. They have their pros and cons but overall, given that top *NA* percentage in a feature is `r AirlinesEncoded %>% is.na() %>% colMeans() %>% as_tibble() %>% arrange(desc(value)) %>% top_n(1)`, we settled on simply dropping rows containing *NAs* if necessary. They constitute roughly `r round(mean(AirlinesEncoded %>% is.na() %>% rowSums() > 0), 2)` of all observations, so we would still have `r (AirlinesEncoded %>% drop_na() %>% dim())[1]` observations left to work with. That should be enough. 

However we will hold off with the actual *NA* dropping until feature selection is finalized. Take for example the feature `ScheduleNote` - out of all the rows that would be dropped due to data missingness, almost half is caused by this feature only. That is, if `ScheduleNote` turns out to be a redundant feature and we discard it, then instead of dropping $8\%$ of all rows, we would be dropping only $5\%$.

## Exploratory Data Analysis

**Describe the ideas of this section. To be done, not urgent...**

Data summary
```{r}
Airlines %>% summary()
```
### Feature Selection

The more is not always the better. Every model has a certain computational complexity that increases with the number of additional explanatory variables. The feature selection in a pre-modeling environment serves identifying groups of variables which carry repeated or very similar informational value. *Filtering feature selection methods* allow one to discard redundant features in a model independent way. By reducing the number of variables they simplify the model and increase it's interpretability. It is also a step which tackles multicollinearity (high linear codependency of explanatory variables) which kills stability and predictive power of some models.

## Categorical Variables

### The information Value

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


## The Continous Variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Decide which Continuous Variable to Use

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Data Binning

### The Categorical Variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### The Continuous variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# The Logisic Regresssion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# The performance of the Model

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Validation of the Model

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


## Monte Carlo Cross Validation


Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# The Challenger Models

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Neural Network

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Another logistic regression: logistic 2

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Bibliography