---
title: |
  | \LARGE \textsf The Selection of a Model for Airlines Customer Satisfaction
  | \vspace{4ex}
author: "Joanna Krężel, Anna Matysek, Piotr Mikler, Adam Szczerba"
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "The project aims to analyze the data about satisfaction of Investico Airlines passengers. Having customer-granular observations about the cruise and a reported satisfaction level for particular aspects of the flight we try to fit models which predict whether they are satisfied with the service or not. The binary classification models used during the project are {} {} {}, out of which our recommendation is {} based on {}. This document describes the process we undertook and presents the results of data preprocessing, model selection and model validation."
documentclass: article 
classoption:
  - 12pt
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
header-includes:
    \usepackage{titling}
    \pretitle{\begin{center}
    \vspace{-7ex}
    \includegraphics[height=40mm]{img/logo3AGH.png}\\
    \vspace{5ex}
    {\large \bf \textsf{AGH UNIVERITY OF SCIENCE AND TECHNOLOGY}}\\
    {\large \bf \textsf{FACULTY OF APPLIED MATHEMATICS}}\\
    \vspace{10ex}
    }
---

<!--
Alternative, use the mgrwms template:
https://code.google.com/archive/p/mgrwms/downloads
http://zasoby.open.agh.edu.pl/~12sjkaminski/indexba7d.html?q=pl/content/klasy-agh
-->


\newpage

\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(forcats)
library(InformationValue)
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
library(ROCR)
library(modelr)
library(purrr)
library(httr)
library(caret)
library(woeBinning)

# SETUP
# Some config variables
bDownloadFromKaggle <- FALSE  # fetch data from Kaggle when compiling .Rmd
bRunComputationallyHeavyStuff <- FALSE # set ```{r eval=bRunComputationallyHeavyStuff}```
                                       # to control execution of a problematic cell from                                          # here

apiCredentials <- list(username = "piotrmikler",
                       key = "8453a8073400c47671d87e6a1b0fe7da")  # leave as-is
setwd("D:/Classes/Mgr/Semestr_3/Quantitative_Analysis_for_Managerial_Decisions/Airlines") # change for your main directory
```

\newpage

# Introduction

_Tbd..._

# Data

```{r}
fetch_from_kaggle <- function(sOutputPath, apiCredentials){
  #' Download the project dataset from Kaggle API
  #' 
  #' @param sOutputPath path-like string, location where to save data
  #'
  #' @param apiCredentials Kaggle API credentials, list of username and key

  sZipFile <- file.path(sOutputPath, 'Dataset.zip')
  apiResponse <- httr::GET("https://www.kaggle.com/api/v1/datasets/download/sjleshrac/airlines-customer-satisfaction",
                           httr::authenticate(apiCredentials$username, 
                                              apiCredentials$key, 
                                              type="basic"))
  # Download and unzip
  download.file(apiResponse$url, 
                destfile = sZipFile, 
                mode="wb")
  unzip(zipfile = sZipFile, 
        file = 'Invistico_Airline.csv',
        exdir = file.path(sOutputPath, "Data"))
  unlink(sZipFile)
}

if (bDownloadFromKaggle){
  fetch_from_kaggle(sOutputPath = getwd(),
                    apiCredentials = apiCredentials)
}

sFileName <- file.path(getwd(), '/Data/Invistico_Airline.csv')

AirlinesRaw <- read_csv(sFileName,
                        col_types = "fffdffdffffffffffffffdd")
```

The data is downloaded from [www.kaggle.com](https://www.kaggle.com/sjleshrac/airlines-customer-satisfaction?fbclid=IwAR1azFXqBkqKfah7ZTJ16CcR6S-zYpdQMJ7z7hlXcrEtzcJgh6nlN_4Bu8Y) and delivered by an airline organization. The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values. Also the airlines need to know on which aspect of the services offered by them have to be emphasized more to generate more satisfied customers. The data consists of 129880 rows and 23 columns.

Below we list all column names with explanations of the variables' meaning.
For categorical variables describing satisfaction level, $0$ means _Not Available_ and reflects situation in which the passenger did not provide a rating.

Feature | Description | Values
--------|-------------|-------
Satisfaction | Airline satisfaction level | satisfied/dissatisfied
Gender | Gender of the passenger | male/female
Customer type | The customer type | loyal / disloyal customer
Age | The age of a passenger | [7; 85] years
Type of travel | Purpose of the flight | personal / business travel
Class | Travel class in the plane | business / eco / eco plus
Flight distance | The flight distance of the journey | [50; 6951] miles
Seat comfort | Satisfaction level of seat comfort | {0-5}
Departure/arrival | Satisfaction level of departure/arrival time | {0-5}
Food and drink | Satisfaction level of food and drink | {0-5}
Gate location | Satisfaction level of gate location | {0-5}
Inflight WiFi service | Satisfaction level of the inflight wifi service | {0-5}
Inflight entertainment | Satisfaction level of inflight entertainment | {0-5}
Online support | Satisfaction level of online support | {0-5}
Ease of online booking | Satisfaction level of online booking | {0-5}
On-board services | Satisfaction level of on-board service | {0-5}
Leg room service | Satisfaction level of leg room service | {0-5}
Baggage handling | Satisfaction level of baggage handling | {0-5}
Checkin service |Satisfaction level of check-in service | {0-5}
Cleanliness | Satisfaction level of cleanliness | {0-5}
Online boarding | Satisfaction level of online boarding | {0-5}
Departure delay in minutes | Delay upon departure | [0; 1592] minutes
Arrival delay in minutes | Delay upon arrival | [0; 1584] minutes


## Exploratory Data Analysis

**Describe the ideas of this section. To be done, not urgent...**


## Data Preprocessing

As both academia and business point out, the data-related operations typically constitute about $80\%$ of the whole effort of a modeling pipeline. The performance of any model is heavily driven by the quality of it's inputs. It can be easily proven in a simple trial by combat that even a suboptimal model running on high quality data can oftentimes bring a sophisticated one with poor inputs to it's knees. For that reason it is of utmost importance to pay extra care and attention to the data which is fed to the decision making models.

During data preprocessing step we will gain insight about data statistics and information it conveys. First, we'll deal with NAs and outliers. Then we will refactor the features with variable binning and select a subset of features which are likely to display predictive power for our problem. In the end we will encode the variables and prepare a train-test split for model development.

```{r}
# Encoding Categorical Binary Features
sCategorialColnames <- c('satisfaction', 'Gender', 'Customer Type', 'Type of Travel')

Airlines <-
  AirlinesRaw %>% # Encode as 1 or 0 (Yes or No), store under new column names
  mutate(IsSatisfied = as_factor(ifelse(satisfaction == "satisfied", 1, 0)),
         IsFemale = as_factor(ifelse(Gender == "Female", 1, 0)),
         IsLoyal = as_factor(ifelse(`Customer Type` == "Loyal Customer", 1, 0)),
         IsPersonalTravel = as_factor(ifelse(`Type of Travel`== "Personal Travel", 1, 0))) %>%
  select(-all_of(sCategorialColnames)) %>% # drop old columns
  rename("FlightDistance" = "Flight Distance", # rename long column names
         "SeatNote" = "Seat comfort", 
         "ScheduleNote" = "Departure/Arrival time convenient",
         "FoodNote" = "Food and drink",
         "GateNote" = "Gate location",
         "WifiNote" = "Inflight wifi service",
         "EntertainmentNote" = "Inflight entertainment",
         "eSupportNote" = "Online support",
         "eBookingNote" = "Ease of Online booking",
         "ServiceNote" = "On-board service",
         "LegRoomNote" = "Leg room service",
         "BaggageNote" = "Baggage handling",
         "CheckInNote" = "Checkin service",
         "CleanNote" = "Cleanliness",
         "eBoardingNote" = "Online boarding",
         "DepartureDelay" ="Departure Delay in Minutes",
         "ArrivalDelay" = "Arrival Delay in Minutes") %>%
  mutate_at(vars(matches("Note")),  # in all "Note" features
            ~ replace(., which(.==0), NA)) %>%  # replace 0s with NAs
  mutate_at(vars(matches("Note")),  # in all "Note" features
            droplevels)  # drop empty factor levels (0s are no longer present)
```


### Missing data treatment


```{r NA_structure}
# Summarize NAs

NA_structure <-
  Airlines %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>%
  mutate(value = is.na(value)) %>%
  group_by(Feature) %>%
  summarise(NA_count = sum(value),
            pct_of_data = paste0(round(100*sum(value)/length(value), 2), "%")) %>%
  filter(NA_count > 0) %>%
  arrange(desc(NA_count))

knitr::kable(NA_structure,
             caption= "NA breakdown per feature. NAs span a small portion of data \\label{tab:NA_structure}")
```

After examining the data it seems we don't have any critical issue related to missing values. *NAs* are present in $12$ variables, but they constitute a minuscule portion of a very large dataset (see fig. \ref{tab:NA_structure}). We considered employing an imputation strategy based on median, but given that NAs constitute roughly `r round(mean(Airlines %>% is.na() %>% rowSums() > 0), 2)` of all observations, even if we drop them we would still have `r (Airlines %>% drop_na() %>% dim())[1]` observations left to work with. Based on that we decided not to introduce imputed values to the dataset, but rather work with pure data. 

```{r}
Airlines <- drop_na(Airlines)
```


### Feature Engineering

The more is not always the better. Feature engineering is a pre-modeling stage which serves identifying features which are significant and filtering out the ones that are not. *Filtering feature selection methods* allow one to discard redundant features in a model independent way. By reducing the number of variables or discretizing them we simplify the model and increase it's interpretability. It is also a step which tackles multicollinearity (high linear codependency of explanatory variables) which kills stability and predictive power of some models.

Across the following sections we are going to introduce a few additional modifications of variables to the dataset and see if it makes sense to keep them. Since we have a big share of $0s$ in column `DepartureDelay`, we wanted to check if it makes sense to include a binary variable $IsDelayed$ as a predictor. We are also going to try discretizing `Age`, `DepartureDelay` and `FlightDistance` continuous variables into bins based on Weight of Evidence metric and evaluate their predictive power.

#### Continuous Features

We have four continuous variables in our dataset: `Age`, `DepartureDelay`, `ArrivalDelay` and `FlightDistance`. We start the analysis by analyzing their codependence structure.
We note a high linear relationship between `ArrivalDelay` and `DepartureDelay`, visible both in the correlation matrix  (fig. \ref{fig:Multicollinearity}) and on figure \ref{fig:Delays_Collinearity}. We can safely drop `ArrivalDelay`, since it doesn't introduce new information and additionally contaminates the dataset with NAs.
```{r Multicollinearity, echo=FALSE, fig.width=4, fig.height=4, fig.cap="\\label{fig:Multicollinearity} Pearson correlation matrix only detects strong collinearity in Delays."}
multicorr <- 
  Airlines %>% 
  select(all_of(c('Age', 'DepartureDelay', 'ArrivalDelay', 'FlightDistance'))) %>%
  drop_na() %>%
  cor() %>%
  ggcorrplot(lab = T)

multicorr
```


```{r Delays_Collinearity, echo=FALSE, fig.width=3, fig.height=3, fig.cap="\\label{fig:Delays_Collinearity} Strong linear relationship between departure delay and arrival delay allows to drop one of them from the dataset."}
c <- cor(x = drop_na(Airlines)$DepartureDelay, y = drop_na(Airlines)$ArrivalDelay)
Airlines %>% drop_na() %>% 
  ggplot(aes(x = DepartureDelay, y = ArrivalDelay)) +
  geom_point() + geom_smooth()

Airlines$ArrivalDelay <- NULL
```

Next, we're going to examine the loess estimator of satisfaction as a function of the remaining continuous variables to see if any of them looks flat enough to raise suspicion regarding it's utility. Flatness of loess implies that average satisfaction does not change in the explanatory variable, hence the explanatory variable doesn't convey much information. 
In our case, this is not visible on the figure \ref{fig:loess}, so we cannot discard any feature based on that.  Note, the plot has been generated on a randomized data sub-sample for computational complexity reduction. We made sure however to take a sample large enough, so that the standard errors are tamed, and to ensure the relationship shape is stable regardless of the random seed chosen.

```{r, fig.width=5, fig.height=8, fig.caption = "Loess estimator for IsSatisfied as function of continuous features from the dataset.\\label{fig:loess}"}

set.seed(127)
Airlines_numeric <- 
  Airlines %>% 
  mutate_all(as.character) %>%
  mutate_all(as.numeric) %>%
  slice_sample(n = 5000)

p1 <- qplot(data = Airlines_numeric, Age, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("Age")
p2 <- qplot(data = Airlines_numeric, FlightDistance, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("FlightDistance")
p3 <- qplot(data = Airlines_numeric, DepartureDelay, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("DepartureDelay")

grid.arrange(p1, p2, p3, ncol=1)
```

Now we are going to look at possible binnings of our features. We'll use `woeBinning::woe.binning` function which chooses the binning to maximize the information value of the feature. If the optimized binning will yield $IV < 0.1$, we will discard the variable. Otherwise we'll analyze the bins to ensure they are not over-optimized to an unreasonable degree. Based on loess plot shapes/regimes the expectation is to have not more than four bins for `Age` and `FlightDistance`, and a maximum of two bins for `DepartureDelay`.  

```{r, results='asis'}
cols_to_bin <- Airlines %>% select(where(is.double)) %>% colnames
cutpoints <- Airlines %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', '1.Rate', 'WOE', "IV")

for (i in 1:length(cols_to_bin)){
  woetable <- binnings[i]
  available_cols <- colnames(woetable[[1]])

  cat(names(woetable[1]))
  woetable[[1]] %>% 
    select(all_of(intersect(display_cols, available_cols))) %>%
    knitr::kable() %>%
    print()
}
```

From the WOE tables above we see that `DepartureDelay` is a variable of low predictive power, hence we won't use it in modelling. For the other variables, as the data binning chosen by the algorithm seems reasonable given the ex-ante expectations, we're going to keep them.

```{r}
Airlines_binned <- Airlines %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  select(-all_of(c("Age", "FlightDistance", "DepartureDelay"))) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  as_tibble()

levels(Airlines_binned$FlightDistance) <- c("L", "M", "H", NA)
levels(Airlines_binned$Age) <- c("u30", "30s", "40s50s", "60plus", NA)
```

#### Ordinal & Categorical Features

The main challenge of the data preparation in this dataset is the proper treatment of passenger notes.
Take for example the `SeatNote` feature which is a customer note describing their satisfaction level with the seating arrangement. One could ask himself the following questions:

* What did the passenger have in mind? Satisfaction with their seat location? Seat comfort? Possibility of choosing the seat?
* Does $`SeatNote`=3$ imply a negative attitude towards a service? Or it's a moderate 'OK'?
* Is the satisfaction *"difference"* between notes $3$ and $2$ the same as between notes $5$ and $4$?
* Is a note $`SeatNote`=5$ given $`Class`=`Eco`$ the same as $`SeatNote`=5$ given $`Class`=`Business`$?

The same point is valid for any note-type variable in the dataset. The issue boils down to the problem that there is **no universal "unit" of satisfaction**. It is just as non-trivial to measure it as to predict it - simply because everyone perceives satisfaction in a subjective way. Our problem has an additional layer of complexity since we don't have information how precisely the survey questions have been described to the customers - so even if we *did* have some carefully designed satisfaction unit, we cannot be sure if all respondents referred to the same aspects of service when filling out the survey.

We aim to overcome this problem, by binning notes into wider classes, depending on how well they explain and affect the overall satisfaction. Since we have a lot of 5-leveled `Note` factors, there's a strong suspicion that in such large set there must exist some adjacent levels such that overall satisfaction is invariant to displacements in that group of levels. In other words, we could collapse notes of $1, 2 \& 3$ to one group if they carried similar information. Hence we will again let `woe.binning` automatically select bins and then verify the result.

Since there are $12$ `Note` variables, we will only display one of the WOE tables as an example. However for all it has been verified that **adjacent** levels have been binned, so the binning is plausible, and the $IV$ of the newly binned features are above $0.1$.
```{r, results='asis'}
cols_to_bin <- Airlines_binned %>% select(c(ends_with('Note'))) %>% colnames
cutpoints <- Airlines_binned %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', 'WOE', "IV")

Airlines_binned <- Airlines_binned %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  as_tibble() %>%
  select(-all_of(cols_to_bin)) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  mutate_at(vars(matches("Note")), ~as_factor(ifelse(grepl("1", .), "L",
                                                 ifelse(grepl("5", .), "H",
                                                        "M"))))
binnings$`WOE Table for eBookingNote` %>% knitr::kable()
```

Next, we will check the information value for other categorical variables. They are binary, so binning has not been applied to them in the earlier step. The table \ref{tab:CategoricalFeatureSelection} presents features and their IV. We see that there are features with IV smaller than $0.1$ - we are going to drop those from the dataset. 

Lastly, a brief look at the spearman rank correlation matrix (fig. \ref{fig:spearman_cor}) shows that there are no highly correlated "note" features among ordinal variables in the dataset.

```{r CategoricalFeatureSelection, caption="Categorical features with Information Value smaller than 0.1\\label{tab:CategoricalFeatureSelection}"}
sCategoricalFeatures <- 
  Airlines_binned %>%
  colnames() %>% 
  setdiff(c("IsSatisfied"))

output <- data.frame(varName = character(0),
                     IV = numeric(0))  # create empty data frame

for (colname in sCategoricalFeatures) {
   wt <- WOETable(Airlines_binned[[colname]], 
                  Airlines_binned$IsSatisfied, 
                  valueOfGood = 1)
   
   tmp  <- data.frame(varName = colname, 
                      IV = sum(wt$IV))
   
   output <- rbind(output, tmp)
}

vars_to_drop <- 
  output %>% arrange(IV) %>% filter(IV<0.1) %>% 
  pull(varName) 

table_ <- output %>% arrange(IV) %>% filter(IV < 0.1)
knitr::kable(arrange(output, IV), 
             caption = "Information Value for all variables.\\label{tab:CategoricalFeatureSelection}")
```

```{r}
Airlines_binned <- Airlines_binned %>% select(-all_of(c(vars_to_drop)))
```

```{r CategoricalSpearman,  echo=FALSE, fig.width=7,fig.height=6,fig.cap="\\label{fig:spearman_cor} Spearman correlation shows no significant colinear relationships in ordinal variables"}
spearman_cor <-
  Airlines_binned %>%
  select(ends_with("Note")) %>%
  mutate_all(~as_factor(ifelse(. == "L", 0,
                               ifelse(. == "M", 1, 2)))) %>%
  mutate_all(as.numeric) %>%
  cor(method='spearman')

spearman_cor %>%
  ggcorrplot(lab=T)
```

### Feature Encoding

Some machine learning algorithms require numerical data, so we considered **ordinal encoding** and **dummy encoding** to transform our data.

We ran the following thought experiment to determine which encoding to employ. Say we use ordinal encoding and assign numbers to each factor level. We could encode `Class` this way and assign a mapping like: {'Eco': 1, 'EcoPlus': 2, 'Business': 3}. This type of representation ensures the quality of the service is properly represented in the numeric data, but the question is whether this translates the same to the overall satisfaction? Yes, the business class is clearly more comfortable to travel in, but the *expectations* (the baseline) of business-class passengers will also be quite higher than the expectations of say, passengers in the economic class.

We chose to employ dummy encoding to encode `Class` - to avoid making assumptions about baseline satisfaction criteria across different passenger classes. For `Note` features however this problem is non-existent, since a higher note should correspond to higher satisfaction for any rational passenger. Here to avoid inflating the dataset with additional $4\cdot14 - 14 = 42$ sparse binary columns we will stick to the original ordinal encoding. This choice nonetheless should be revisited and controlled once we reach the stage of model choice and model fitting.

The resulting, encoded dataframe looks the following way:
```{r}
# Encode Class feature
Airlines_binned <- Airlines_binned %>% 
  mutate(Class = as_factor(str_replace(Class, " ", ""))) # Eco Plus -> EcoPlus

vars_to_encode <-
  Airlines_binned %>% 
  select(c(ends_with('Note'), 'Age', 'FlightDistance', 'Class')) %>%
  colnames()

formula_string <- paste0("~ ", paste(vars_to_encode, collapse=" + "))

AirlinesEncoded_binned <- 
  dummyVars(as.formula(formula_string), data = Airlines_binned, 
            fullRank = TRUE, drop2nd = TRUE) %>%  # ensure 1 is dropped
  predict(newdata = Airlines_binned) %>%  # Execute the split
  as_tibble() %>%
  bind_cols(Airlines_binned) %>% # Join dummy features with the original dataset
  select(-all_of(vars_to_encode)) %>%  # drop old 'Class' feature
  mutate_all(as_factor) # ensure all are factors

AirlinesEncoded_binned %>% glimpse()
```  



# A train-test split of data
```{r}
# First we build the random indexes:
N <- nrow(Airlines_binned)
train_index <- sample(1:N, 0.8 * N)
test_index  <- setdiff(1:N, train_index)
# Then we build the training and test data set:
Airlines_binned_train <- Airlines_binned[train_index, ]
Airlines_binned_test  <- Airlines_binned[test_index, ]
AirlinesEncoded_binned_train <- AirlinesEncoded_binned[train_index, ]
AirlinesEncoded_binned_test  <- AirlinesEncoded_binned[test_index, ]
```

# Baseline Model

Before jumping straight into cutting-edge mathematical models, sometimes it can be very teaching to fit a simple model and analyze how well it performs on the data. It sets ground zero for any more complicated model that follow and allows to adjust the own expectations.


## Fitting and performance

```{r}
library(keras)
X_train <- 
  AirlinesEncoded_binned_train %>% select(-IsSatisfied)

y_train <-
  AirlinesEncoded_binned_train %>% select(IsSatisfied)

X_test <- 
  AirlinesEncoded_binned_test %>% select(-IsSatisfied)

y_test <- 
  AirlinesEncoded_binned_test %>% select(IsSatisfied) 

X_train <- X_train %>% 
  mutate_all(as.character) %>% mutate_all(as.numeric) %>% mutate_all(scale)
y_train <- y_train %>%
  mutate_all(as.character) %>% mutate_all(as.numeric)
X_test <- X_test %>% 
  mutate_all(as.character) %>% mutate_all(as.numeric) %>% mutate_all(scale)
y_test <- y_test %>%
  mutate_all(as.character) %>% mutate_all(as.numeric)

model <- keras_model_sequential()
model %>%
  layer_dense(units = 1, activation = 'sigmoid')

model %>% compile(
  optimizer = 'adam', 
  loss = 'binary_crossentropy',
  metrics = c('accuracy'))

model %>% fit(as.matrix(X_train), as.matrix(y_train),
              validation_split= 0.3, batch_size=50,
              epochs = 10, verbose = 2)

preds <- model %>% predict(as.matrix(X_test)) > 0.5
preds <- preds %>% as.numeric()


confusionMatrix(table(preds, unlist(y_test, use.names = F)))
```

## Validation

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# The Challenger Models

## Random Forest

### Fitting and performance
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Validation
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Logistic Regression

### Fitting and performance
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Validation
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## State-of-the-art: Neural Network model

### Fitting and performance
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Validation
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Bibliography