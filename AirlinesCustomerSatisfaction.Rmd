---
title: |
  | \LARGE \textsf The Selection of a Model for Airlines Customer Satisfaction
  | \vspace{4ex}
author: "Joanna Krężel, Anna Matysek, Piotr Mikler, Adam Szczerba"
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "The project aims to analyze the data about satisfaction of Investico Airlines passengers. Having customer-granular observations about the cruise and a reported satisfaction level for particular aspects of the flight we try to fit models which predict whether they are satisfied with the service or not. The binary classification models used during the project are Adaline, Logistic regression, Random Forest and a Neural Network; out of which our recommendation is Logistic Regression based on explainability and high performance metrics. This document describes the process we undertook and presents the results of data preprocessing, model selection and model validation."
documentclass: article 
classoption:
  - 12pt
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
header-includes:
    \usepackage{titling}
    \pretitle{\begin{center}
    \vspace{-7ex}
    \includegraphics[height=40mm]{img/logo3AGH.png}\\
    \vspace{5ex}
    {\large \bf \textsf{AGH UNIVERITY OF SCIENCE AND TECHNOLOGY}}\\
    {\large \bf \textsf{FACULTY OF APPLIED MATHEMATICS}}\\
    \vspace{10ex}
    }
---

<!--
Alternative, use the mgrwms template:
https://code.google.com/archive/p/mgrwms/downloads
http://zasoby.open.agh.edu.pl/~12sjkaminski/indexba7d.html?q=pl/content/klasy-agh
-->


\newpage

\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(forcats)
library(InformationValue)
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
library(ROCR)
library(modelr)
library(purrr)
library(httr)
library(caret)
library(woeBinning)
library(htmlTable)
library(dplyr)
library(magrittr)
library(tensorflow)
library(keras)
library(ROCR)
library(naniar)
# SETUP
# Some config variables
bDownloadFromKaggle <- FALSE  # fetch data from Kaggle when compiling .Rmd
bRunComputationallyHeavyStuff <- FALSE # set ```{r eval=bRunComputationallyHeavyStuff}```
                                       # to control execution of a problematic cell from                                          # here

apiCredentials <- list(username = "piotrmikler",
                       key = "8453a8073400c47671d87e6a1b0fe7da")  # leave as-is
setwd(".") # change for your main directory
```

\newpage

# Introduction

During the project we undertook a problem of an Airline Company, which surveyed it's passengers on different aspects of the journey and would like to make inference about parts of their service which impact the overall client satisfaction.

The document presents our approach to answering that question. The first chapter describes the origin and treatment of the data. In the second chapter we talk about different binary classification models, fit and validate them. In the last part of the document their performance is compared on an unseen test set and we conclude with a model recommendation and answer the posed question based on the findings of the project. 

# Data

```{r}
fetch_from_kaggle <- function(sOutputPath, apiCredentials){
  #' Download the project dataset from Kaggle API
  #' 
  #' @param sOutputPath path-like string, location where to save data
  #'
  #' @param apiCredentials Kaggle API credentials, list of username and key

  sZipFile <- file.path(sOutputPath, 'Dataset.zip')
  apiResponse <- httr::GET("https://www.kaggle.com/api/v1/datasets/download/sjleshrac/airlines-customer-satisfaction",
                           httr::authenticate(apiCredentials$username, 
                                              apiCredentials$key, 
                                              type="basic"))
  # Download and unzip
  download.file(apiResponse$url, 
                destfile = sZipFile, 
                mode="wb")
  unzip(zipfile = sZipFile, 
        file = 'Invistico_Airline.csv',
        exdir = file.path(sOutputPath, "Data"))
  unlink(sZipFile)
}

if (bDownloadFromKaggle){
  fetch_from_kaggle(sOutputPath = getwd(),
                    apiCredentials = apiCredentials)
}

sFileName <- file.path(getwd(), '/Data/Invistico_Airline.csv')

AirlinesRaw <- read_csv(sFileName,
                        col_types = "fffdffdffffffffffffffdd")
```

The data is downloaded from [www.kaggle.com](https://www.kaggle.com/sjleshrac/airlines-customer-satisfaction?fbclid=IwAR1azFXqBkqKfah7ZTJ16CcR6S-zYpdQMJ7z7hlXcrEtzcJgh6nlN_4Bu8Y) and delivered by an anonimized airline organization, called *Invistico Airlines* for the purpose of the project. The dataset consists of customer-granular information about them, their flight and survey votes given for specific parts of the service. The variable of interest is the binary *satisfaction* answer, which denotes whether a customer was overall satisfied or dissatisfied with their flight. 
To help us do that, we have 129,880 instances of 22 explanatory variables, which we describe below.

Feature | Description | Variable type
--------|-------------|-------
Satisfaction | Overall satisfaction | factor, 2 levels
Gender | Gender of the passenger | factor, 2 levels
Customer type | Loyalty of the passenger | factor, 2 levels
Age | The age of a passenger | continuous
Type of travel | Flight purpose | factor, 2 levels
Class | Travel class in the plane | factor, 3 levels
Flight distance | Distance of the journey | continuous
Seat comfort | Survey note for seat comfort | factor, 5 levels
Departure/arrival | Survey note for departure/arrival time convenience | factor, 5 levels
Food and drink | Survey note for food and drinks | factor, 5 levels
Gate location | Survey note for gate location | factor, 5 levels
Inflight WiFi service | Survey note for the inflight wifi | factor, 5 levels
Inflight entertainment | Survey note for inflight entertainment | factor, 5 levels
Online support | Survey note for online support | factor, 5 levels
Ease of online booking | Survey note for online booking | factor, 5 levels
On-board services | Survey note for on-board service | factor, 5 levels
Leg room | Survey note for leg room/space | factor, 5 levels
Baggage handling | Survey note for baggage handling | factor, 5 levels
Checkin service | Survey note for check-in service | factor, 5 levels
Cleanliness | Survey note for cleanliness | factor, 5 levels
Online boarding | Survey note for online boarding | factor, 5 levels
Departure delay | Delay upon departure | continuous
Arrival delay | Delay upon arrival | continuous


## Exploratory Data Analysis

Before data wrangling and modelling, it is good to get an overview of our data set and verify whether the variables meet a commmon sense. We start by summarizing the dataframe to see a handful of basic statistics. For each quantitative variable we get the min, max, mean, median and the IQR. For each categorical variable on the other hand we see the number of observations per category.
```{r,echo=FALSE}
summary(AirlinesRaw)
```

It's worth noting at this stage that for survey answers describing customer satisfaction level, $0$ means *Not Available* and reflects a situation in which the passenger did not provide an answer.

Looking at the data summary we don't see any immediate problems, and overall the data statistics seem to be reasonable. It is noteworthy that the numbers of satisfied and dissatisfied customers in the dataset are comparable, which means that we're not working with an imbalanced binary classification problem. 

Moreover, we have almost the same number of males and females, and of business and eco class passengers. However, we have significantly more opinions from loyal customers than from disloyal ones. We note that loyal customers might give higher survey answers because of e.g. benefits due to loyalty programs. What we also notice is the fact that there are missing values in some columns, which we will take care about in the following chapters.

Additionally, we visualize histograms of features to view the distribution of continuous variables.

```{r fig.width=7}
AirlinesRaw %>%
  select(all_of(c("Age", "Flight Distance", "Departure Delay in Minutes", "Arrival Delay in Minutes"))) %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>%
  ggplot(aes(x=value, fill=Feature)) + geom_histogram(bins=20) + facet_wrap(~Feature, scales = "free") + theme_bw() + scale_fill_brewer(palette="Dark2")
```
We see that:

* departure and arrivals delays are mostly zero, or small,
* the most common flight distance is around 2,000 kilometers,
* the age varies between very small children to elderly in their 80s, but the biggest number of customers are between their 20s and 60s.

Now, as when we have an initial overview of our data set, we move to data preprocessing. 

## Data Preprocessing

As both academia and business point out, the data-related operations typically constitute about $90\%$ of the whole effort in a modeling pipeline. The performance of any model is heavily driven by the quality of it's inputs. It can be proven in a simple trial by combat that even a suboptimal model running on high quality data can oftentimes bring a sophisticated one with poor inputs to it's knees. For that reason it is of utmost importance to pay extra care and attention to the data which is fed to the decision making models.

During data preprocessing step we will raise questions about the information the data conveys. First, we'll deal with NAs and outliers. Then we will refactor the features using variable binning and select a subset of features displaying predictive power. In the end we will encode the variables and prepare a train-validation split for model development.

```{r}
# Encoding Categorical Binary Features
sCategorialColnames <- c('satisfaction', 'Gender', 'Customer Type', 'Type of Travel')

Airlines <-
  AirlinesRaw %>% # Encode as 1 or 0 (Yes or No), store under new column names
  mutate(IsSatisfied = as_factor(ifelse(satisfaction == "satisfied", 1, 0)),
         IsFemale = as_factor(ifelse(Gender == "Female", 1, 0)),
         IsLoyal = as_factor(ifelse(`Customer Type` == "Loyal Customer", 1, 0)),
         IsPersonalTravel = as_factor(ifelse(`Type of Travel`== "Personal Travel", 1, 0))) %>%
  select(-all_of(sCategorialColnames)) %>% # drop old columns
  rename("FlightDistance" = "Flight Distance", # rename long column names
         "SeatNote" = "Seat comfort", 
         "ScheduleNote" = "Departure/Arrival time convenient",
         "FoodNote" = "Food and drink",
         "GateNote" = "Gate location",
         "WifiNote" = "Inflight wifi service",
         "EntertainmentNote" = "Inflight entertainment",
         "eSupportNote" = "Online support",
         "eBookingNote" = "Ease of Online booking",
         "ServiceNote" = "On-board service",
         "LegRoomNote" = "Leg room service",
         "BaggageNote" = "Baggage handling",
         "CheckInNote" = "Checkin service",
         "CleanNote" = "Cleanliness",
         "eBoardingNote" = "Online boarding",
         "DepartureDelay" ="Departure Delay in Minutes",
         "ArrivalDelay" = "Arrival Delay in Minutes") %>%
  mutate_at(vars(matches("Note")),  # in all "Note" features
            ~ replace(., which(.==0), NA)) %>%  # replace 0s with NAs
  mutate_at(vars(matches("Note")),  # in all "Note" features
            droplevels)  # drop empty factor levels (0s are no longer present)
```


### Missing data treatment

```{r NA_structure}
# Summarize NAs

NA_structure <-
  Airlines %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>%
  mutate(value = is.na(value)) %>%
  group_by(Feature) %>%
  summarise(NA_count = sum(value),
            pct_of_data = paste0(round(100*sum(value)/length(value), 2), "%")) %>%
  filter(NA_count > 0) %>%
  arrange(desc(NA_count))

knitr::kable(NA_structure,
             caption= "NA breakdown per feature. NAs span a small portion of data \\label{tab:NA_structure}")
```

After examining the data missingness it seems we don't have any critical issue related to missing values. *NAs* are present in $12$ variables, but they constitute a minuscule portion of a very large dataset (see table \ref{tab:NA_structure}). We considered employing an imputation strategy based on median, but given that roughly `r round(mean(Airlines %>% is.na() %>% rowSums() > 0), 2)` of all rows contain an *NA*, and even if we drop them we would still have `r (Airlines %>% drop_na() %>% dim())[1]` observations left to work with. Based on that we decided not to introduce imputed values to the dataset, but rather work with pure data.

```{r}
#gg_miss_upset(Airlines)
```


```{r}
Airlines <- drop_na(Airlines)
```


### Feature Engineering

The more is not always the better. Feature engineering is a pre-modeling stage which serves identifying features which are significant and filtering out the ones that are not. *Filtering feature selection methods* allow one to discard redundant features in a model independent way. By reducing the number of variables or discretizing them we simplify the model and increase it's interpretability. It is also a step which tackles multicollinearity which kills stability and predictive power of some models.

#### Continuous Features

We have four continuous variables in our dataset: `Age`, `DepartureDelay`, `ArrivalDelay` and `FlightDistance`. We start visualizing their co-dependence structure.
We note a high linear relationship between `ArrivalDelay` and `DepartureDelay`, visible both in the correlation matrix  (fig. \ref{fig:Multicollinearity}) and on their scatterplot (fig. \ref{fig:Delays_Collinearity}). Therefore we can safely drop `ArrivalDelay`, since it doesn't introduce much new information.
```{r Multicollinearity, echo=FALSE, fig.width=4, fig.height=4, fig.cap="\\label{fig:Multicollinearity} Pearson correlation matrix only detects strong collinearity in Delays."}
multicorr <- 
  Airlines %>% 
  select(all_of(c('Age', 'DepartureDelay', 'ArrivalDelay', 'FlightDistance'))) %>%
  drop_na() %>%
  cor() %>%
  ggcorrplot(lab = T)

multicorr
```


```{r Delays_Collinearity, echo=FALSE, fig.width=3, fig.height=3, fig.cap="\\label{fig:Delays_Collinearity} Strong linear relationship between departure delay and arrival delay allows to drop one of them from the dataset."}
c <- cor(x = drop_na(Airlines)$DepartureDelay, y = drop_na(Airlines)$ArrivalDelay)
Airlines %>% drop_na() %>% 
  ggplot(aes(x = DepartureDelay, y = ArrivalDelay)) +
  geom_point() + geom_smooth()

Airlines$ArrivalDelay <- NULL
```

Next, we're going to examine the loess estimator of satisfaction as a function of the remaining continuous variables to see if any of them looks flat enough to raise suspicion regarding it's utility. Flatness of loess implies that average satisfaction does not change in the explanatory variable, hence the explanatory variable doesn't convey much information. 
In our case, this is not visible on the figure \ref{fig:loess}, so we cannot discard any feature based on that.  Note, the plot has been generated on a randomized data sub-sample for computational complexity reduction. We made sure however to take a sample large enough, so that the standard errors are tamed, and to ensure the relationship shape is stable regardless of the random seed chosen.

```{r, fig.width=5, fig.height=8, fig.caption = "Loess estimator for IsSatisfied as function of continuous features from the dataset.\\label{fig:loess}"}

set.seed(127)
Airlines_numeric <- 
  Airlines %>% 
  mutate_all(as.character) %>%
  mutate_all(as.numeric) %>%
  slice_sample(n = 5000)

p1 <- qplot(data = Airlines_numeric, Age, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("Age")
p2 <- qplot(data = Airlines_numeric, FlightDistance, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("FlightDistance")
p3 <- qplot(data = Airlines_numeric, DepartureDelay, IsSatisfied, xlab = "", ylab = "") +
     geom_smooth(method = "loess", se=T) +
     ggtitle("DepartureDelay")

grid.arrange(p1, p2, p3, ncol=1)
```

Now we are going to look at possible binnings of our features. We'll use `woeBinning::woe.binning` function which chooses the binning to maximize the information value of the feature. If the optimized binning will still yield $IV < 0.1$, we will discard the variable. Otherwise we'll analyze the bins to ensure they are not over-optimized to an unreasonable degree. Based on loess plot shapes/regimes the expectation is to have not more than four bins for `Age` and `FlightDistance`, and a maximum of two bins for `DepartureDelay`.  

```{r, results='asis'}
cols_to_bin <- Airlines %>% select(where(is.double)) %>% colnames
cutpoints <- Airlines %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', '1.Rate', 'WOE', "IV")

for (i in 1:length(cols_to_bin)){
  woetable <- binnings[i]
  available_cols <- colnames(woetable[[1]])

  cat(names(woetable[1]))
  woetable[[1]] %>% 
    select(all_of(intersect(display_cols, available_cols))) %>%
    knitr::kable() %>%
    print()
}
```

From the WOE tables above we see that `DepartureDelay` is a variable of low predictive power, hence we won't use it in modelling. For the other variables, as the data binning chosen by the algorithm seems reasonable given the ex-ante expectations, we're going to keep them.

```{r}
Airlines_binned <- Airlines %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  select(-all_of(c("Age", "FlightDistance", "DepartureDelay"))) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  as_tibble()

levels(Airlines_binned$FlightDistance) <- c("L", "M", "H", NA)
levels(Airlines_binned$Age) <- c("u30", "30s", "40s50s", "60plus", NA)
```

#### Ordinal & Categorical Features

The main challenge of the data preparation in this dataset is the proper treatment of passenger survey notes.
Take for example the `SeatNote` feature which is a note describing their satisfaction level with the seat. One could ask himself the following questions:

* Are we sure the passenger had seat comfort in mind? Maybe rather their seat location? Or a possibility of choosing the seat?
* Does $`SeatNote`=3$ imply a negative attitude towards a service? Or it's a moderate 'OK'?
* Is the satisfaction *"difference"* between notes $3$ and $2$ the same as between notes $5$ and $4$?
* Is a note $`SeatNote`=5$ given $`Class`=`Eco`$ the same as $`SeatNote`=5$ given $`Class`=`Business`$?

The same point is valid for any note-type variable in the dataset. The issue boils down to the problem that there is **no universal "unit" of satisfaction**. It is just as non-trivial to measure it as to predict it - simply because everyone perceives satisfaction in a subjective way. Our problem has an additional layer of complexity since we don't have information how precisely the survey questions have been described to the customers - so even if we *did* have some carefully designed satisfaction unit, we cannot be sure if all respondents referred to the same aspects of service when filling out the survey.

We aim to overcome this problem, by binning notes into wider classes, depending on how well they explain and affect the overall satisfaction. Since we have a lot of 5-leveled `Note` factors, we strongly believe that not all levels differ in overall satisfaction contribution. In other words, perhaps we could collapse notes of $1, 2 \& 3$ to one group if they carried similar information. Hence we will again let `woe.binning` automatically select bins and then verify the result.

Since there are $12$ `Note` variables, we will only display one of the WOE tables as an example. However for all it has been verified that **adjacent** levels have been binned, so the binning is plausible, and the $IV$ of the newly binned features are above $0.1$.
```{r}
cols_to_bin <- Airlines_binned %>% select(c(ends_with('Note'))) %>% colnames
cutpoints <- Airlines_binned %>% as.data.frame() %>%
  woe.binning("IsSatisfied", cols_to_bin)

binnings <- woe.binning.table(cutpoints)
display_cols <- c('Final.Bin', 'Total.Count', 'Total.Distr.', '0.Rate', 'WOE', "IV")

Airlines_binned <- Airlines_binned %>% as.data.frame() %>%
  woe.binning.deploy(cutpoints,
                     min.iv.total = 0.1) %>%
  as_tibble() %>%
  select(-all_of(cols_to_bin)) %>%
  rename_with(~gsub(".binned", "", .)) %>%
  mutate_at(vars(matches("Note")), ~as_factor(ifelse(grepl("1", .), "L",
                                                 ifelse(grepl("5", .), "H",
                                                        "M"))))
binnings$`WOE Table for eBookingNote` %>% 
  select(all_of(display_cols)) %>% knitr::kable()
```

The table \ref{tab:CategoricalFeatureSelection} presents all of the binned features and their IVs. We see that there are features with IV smaller than $0.1$ - we are going to drop those from the dataset. 

Lastly, a brief look at the spearman rank correlation matrix (fig. \ref{fig:spearman_cor}) shows that there are no highly correlated "note" features among ordinal variables in the dataset.

```{r CategoricalFeatureSelection, caption="Categorical features with Information Value smaller than 0.1\\label{tab:CategoricalFeatureSelection}"}
sCategoricalFeatures <- 
  Airlines_binned %>%
  colnames() %>% 
  setdiff(c("IsSatisfied"))

output <- data.frame(varName = character(0),
                     IV = numeric(0))  # create empty data frame

for (colname in sCategoricalFeatures) {
   wt <- WOETable(Airlines_binned[[colname]], 
                  Airlines_binned$IsSatisfied, 
                  valueOfGood = 1)
   
   tmp  <- data.frame(varName = colname, 
                      IV = sum(wt$IV))
   
   output <- rbind(output, tmp)
}

vars_to_drop <- 
  output %>% arrange(IV) %>% filter(IV<0.1) %>% 
  pull(varName) 

table_ <- output %>% arrange(IV) %>% filter(IV < 0.1)
knitr::kable(arrange(output, IV), 
             caption = "Information Value for all variables.\\label{tab:CategoricalFeatureSelection}")
```

```{r}
Airlines_binned <- Airlines_binned %>% select(-all_of(c(vars_to_drop)))
```

```{r CategoricalSpearman,  echo=FALSE, fig.width=7,fig.height=6,fig.cap="\\label{fig:spearman_cor} Spearman correlation shows no significant collinear relationships in ordinal variables"}
spearman_cor <-
  Airlines_binned %>%
  select(ends_with("Note")) %>%
  mutate_all(~as_factor(ifelse(. == "L", 0,
                               ifelse(. == "M", 1, 2)))) %>%
  mutate_all(as.numeric) %>%
  cor(method='spearman')

spearman_cor %>%
  ggcorrplot(lab=T)
```

### Feature Encoding

Some machine learning algorithms which we'll use require numerical data, so we performed **dummy encoding** to transform our data. 

That means for each level of factor in our data a separate binary column has been created, and in each factor one of those levels got dropped to avoid having perfect linear relationships.

The resulting, encoded dataframe looks the following way:
```{r}
# Encode Class feature
Airlines_binned <- Airlines_binned %>% 
  mutate(Class = as_factor(str_replace(Class, " ", ""))) # Eco Plus -> EcoPlus

vars_to_encode <-
  Airlines_binned %>% 
  select(c(ends_with('Note'), 'Age', 'FlightDistance', 'Class')) %>%
  colnames()

formula_string <- paste0("~ ", paste(vars_to_encode, collapse=" + "))

AirlinesEncoded_binned <- 
  dummyVars(as.formula(formula_string), data = Airlines_binned, 
            fullRank = TRUE, drop2nd = TRUE) %>%  # ensure 1 is dropped
  predict(newdata = Airlines_binned) %>%  # Execute the split
  as_tibble() %>%
  bind_cols(Airlines_binned) %>% # Join dummy features with the original dataset
  select(-all_of(vars_to_encode)) %>%  # drop old 'Class' feature
  mutate_all(as_factor) # ensure all are factors

AirlinesEncoded_binned %>% glimpse(width = 50)
```  


## A train-test split of data

At this point, we have 2 dataframes, `Airlines_binned` and `AirlinesEncoded_binned` which we'll provide to the use of modellers. Before we start working on model fitting, we reserve a random $20\%$ of observations as a final test set, for comparing model performances.

Modellers will have the remaining $80\%$ available for their needs regarding model fitting and validation.
```{r}
# First we build the random indexes:
N <- nrow(Airlines_binned)
train_index <- sample(1:N, 0.8 * N)
test_index  <- setdiff(1:N, train_index)

# Set "L" as the base level
Airlines_binned <- 
  Airlines_binned %>% 
  mutate_at(vars(ends_with("Note")), ~relevel(. , "L")) %>%
  mutate_at(vars(ends_with("FlightDistance")), ~relevel(. , "L"))

# Then we build the training and test data set:
Airlines_binned_train <- Airlines_binned[train_index, ]
Airlines_binned_test  <- Airlines_binned[test_index, ]
AirlinesEncoded_binned_train <- AirlinesEncoded_binned[train_index, ]
AirlinesEncoded_binned_test  <- AirlinesEncoded_binned[test_index, ]

#Split test data into test_data_splits dataframes
test_data_splits <- 20
Airlines_binned_test_splited <- Airlines_binned_test %>%
                                    split(sample(1:test_data_splits,
                                             nrow(Airlines_binned_test),
                                             replace=T))

AirlinesEncoded_binned_test_splited <- AirlinesEncoded_binned_test %>%
                                    split(sample(1:test_data_splits,
                                             nrow(AirlinesEncoded_binned_test),
                                             replace=T))
```


# Baseline Model

Before jumping straight into cutting-edge mathematical models it can be very teaching to fit a simple one and analyze how well it can perform on a given problem. It sets a ground zero for any more complicated models to follow, and gives an idea about how complex is the problem at hand.

Therefore we will fit a simple adaptive linear neuron ([Adaline](https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html)) to our train set and evaluate it's performance on the test set. It is a binary classification model, designed to find a decision boundary for *linearly separable* datasets. However even if the data is not perfectly separable, we can still fit the algorithm and optimize a chosen loss function - *MSE* in our case.

## Fitting and performance

Conceptually, Adaline is just a single layer neural network. It consists of two parts:

* A linear combination of inputs which is piped through an identity activation function - this is used for learning weights
* A unit step decision function - this part enables performing binary predictions.

Adaline optimizes the weights of that linear combination (a.k.a. the *net input*) to arrive at the best fit on the training set.

We will put our own spin on the adaline algorithm to avoid overfitting. Additional *L2* regularization during training will give the model an incentive to opt for smaller weights, and an *early stopping* callback will halt the training for us, should the validation loss start to plateau and stop decreasing for at least $3$ consecutive epochs.

```{r results='asis'}
X_train <- AirlinesEncoded_binned_train %>%
  select(-IsSatisfied)

y_train <- AirlinesEncoded_binned_train %>%
  select(IsSatisfied)

X_test <- AirlinesEncoded_binned_test %>%
  select(-IsSatisfied)

y_test <- AirlinesEncoded_binned_test %>%
  select(IsSatisfied)

X_train <- X_train %>%
  mutate_all(as.character) %>% mutate_all(as.numeric) %>% mutate_all(scale)
y_train <- y_train %>%
  mutate_all(as.character) %>% mutate_all(as.numeric)
X_test <- X_test %>%
  mutate_all(as.character) %>% mutate_all(as.numeric) %>% mutate_all(scale)
y_test <- y_test %>%
  mutate_all(as.character) %>% mutate_all(as.numeric)

model <- keras_model_sequential()
model %>%
  layer_dense(units = 1,
              kernel_regularizer = regularizer_l2())

model %>% compile(
  optimizer = 'adam',
  loss = 'mean_squared_error',
  metrics = c('accuracy'))

model %>% fit(as.matrix(X_train), as.matrix(y_train),
              validation_data = list(as.matrix(X_test), as.matrix(y_test)),
              batch_size=32,
              epochs = 10, verbose = 1,
              callbacks = list(callback_early_stopping(patience = 3,
                                                       min_delta = 0.01,
                                                       mode="min",
                                                       verbose = 1)))

w <- data.frame(Feature = colnames(X_train), Weight = round(get_weights(model)[[1]], 3))
b <- get_weights(model)[[2]]

```

For the baseline model, as it only serves the purposes of testing the waters, we won't be performing costly cross-validation - but rather only compare key model performance indicators across train and test sets to see what we should expect from the challenger models. Figure \ref{fig:Adaline evaluation} presents that there are no significant performance differences between training and test sets across a selection of metrics, therefore we conclude that the model is able to generalize the learned rules well for unseen data and is not overfitted.

```{r adaline evaluation, fig.width=5, fig.height=3, fig.cap="\\label{fig:Adaline evaluation} Performance metrics for the baseline Adaline model."}
get_metrics <- function(confMatrix){
  sMetrics <- c('Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall')
  metrics <-
    c(confMatrix$overall['Accuracy'],
    confMatrix$byClass[1:2],
    confMatrix$byClass[5:6]) %>% as.data.frame() %>%
    t() %>% t() %>% as_tibble() %>%
    mutate("Metric" = sMetrics)
  colnames(metrics) <- c("Value", "Metrics")
  return(metrics)
}

preds_train <- model %>% predict(as.matrix(X_train)) > 0.5
preds_train <- preds_train %>% as.numeric()

preds_test <- (model %>% predict(as.matrix(X_test))) > 0.5
preds_test <- preds_test %>% as.numeric()

cm_train <- confusionMatrix(table(preds_train, unlist(y_train, use.names = F)))
cm_test <- confusionMatrix(table(preds_test, unlist(y_test, use.names = F)))

metrics_train <- get_metrics(cm_train) %>% mutate("Set" = "Train")
metrics_test <- get_metrics(cm_test) %>% mutate("Set" = "Test")

AUC_baseline_test <- filter(metrics_test, Metrics=="Accuracy")['Value'] %>% as.numeric()
rbind(metrics_train, metrics_test) %>%
  mutate_at(all_of(c("Metrics", "Set")), ~ as_factor(.), levels=c("Train", "Test")) %>%
  ggplot(aes(x=Metrics, fill=Set, y=Value, group=Set)) +
  geom_bar(stat='identity', position='dodge') +
  scale_y_continuous(breaks = round(seq(0, 1, by = 0.1), 1)) +
  theme_bw() + scale_fill_brewer(palette="Accent") +
  coord_flip()
```

In the case of our dataset, since we're working with one-hot-encoded binary features, the magnitudes and signs of fitted adaline weights can be used as proxies for the directions and magnitudes of the influence on the overall satisfaction for a particular feature. More precisely, the weights tell us by how much will the *net input* increase if a particular feature goes from $0$ to $1$. Once the net input (corrected for bias) exceeds $0.5$, this model will predict a satisfied passenger.

The top $3$ influencial feature-note pairs (in a positive and negative sense) according to Adaline model are summarized in table \ref{tab:Adaline_feature_importance}.  One should note that the coefficients translate to overall satisfaction **indirectly**, since e.g. `SeatNote.H=1` implies `SeatNote.M=0`, and these effects offset in the overall net input. However the coefficients should be sufficient to analyze at least the overall importance of particular features.

We see that solid `Entertaiment` and `Seat` notes can severely sway the satisfaction in the positive direction. On the other hand, we have low `Service` notes that moderately drag down the final satisfaction. The biggest negative coefficients that we see are corresponding to `Food`, but they don't seem make intuitive sense. Here a high food note would impact satisfaction more negatively that a low food note. However it is entirely possible that this is due to some missing variable interactions which Adaline is not designed to pick up.
```{r }
w %>%
  arrange(desc(Weight)) %>%
  mutate(Rank = seq(1, dim(w)[1])) %>%
  select(c("Rank", "Feature", "Weight")) %>%
  filter(Rank <= 3 | Rank >= 31) %>%
  knitr::kable(caption = 'Top $3$ positive and negative weights for the Adaline baseline model.\\label{tab:Adaline_feature_importance}')
```

Since Adaline's accuracy on the test set is `r round(AUC_baseline_test, 3)`, we can infer that our data is not hard to separate, or at least it is relatively easy to reach a reasonable performance. For all challenger models we will be aiming to achieve better metrics, which must compensate for additional model complexity.

# The Challenger Models

## Random Forest

### Fitting and performance
As the first modelling option we will try out the Random Forest model. First, we'll consider the below model which includes all of the variables.

```{r}
library(randomForest)
#Define the formula
frm<-IsSatisfied~.
#Fit the model or restore result from file

rf_model_path <- "./random_forest/rf_model.rds"
RanForest1 <- if(file.exists(rf_model_path)){
                  readRDS(rf_model_path)
              }else{
                  set.seed(12428)
                  randomForest(frm,Airlines_binned_train,importance=TRUE) %T>%
                  saveRDS(rf_model_path)
              }

#Investigate the model
RanForest1
```
After running Random Forest with all variables we can already be quite pleased with the results. Only 6.18% of the observations were misqualified thus we obtained a model with 93.82% accuracy.

We denote number of variables sampled at each split as \textbf{mtry}. This parameter allows us to optimize the Random Forest to our problem. By optimize we mean choose \textbf{mtry} such that value of OOB estimate of error rate is the lowest.
```{r}
rf_mtry_path <- "./random_forest/rf_mtry.rds"

mtry <- if(file.exists(rf_mtry_path)){
                  readRDS(rf_mtry_path)
              }else{
                  set.seed(12428)
                  tuneRF(Airlines_binned_train[-2],
                         Airlines_binned_train$IsSatisfied,
                         ntreeTry=500,stepFactor=1.5,improve=0.01,
                         trace=TRUE,plot=TRUE) %T>%
                    saveRDS(rf_mtry_path)
              }
mtry
```
As we see, the value chosen by default is the best one.

Random Forest enables us to determine how important each variable is in the model, two criteria are used:

* MeanDecreaseAccuracy - expresses the accuracy lost by leaving particular variable out of the training set.
* MeanDecreaseGini - measures node impurity at each split, highest purity means that each node contains only elements of a single class.

```{r}
varImpPlot(RanForest1)
```

In terms of accuracy of the model, eBoardingNote, WifiNote and ServiceNote seem to be the least important. The most influential are SeatNote, CheckInNote and eSupportNote. The accuracy of the model could drop significantly if we left them out. As for Gini coefficient, the most important are EntertainmentNote, SeatNote and eBookingNote and the least ServiceNote, WifiNote and eBoardingNote.

SeatNote is placed high on both plots, so we may assume that this variable has strong influence on satisfaction level. The same applies to eSupportNote and EntertainmentNote. While thinking of clients' satisfaction, the airline should definitely focus on providing them with comfortable seats and proper online customer support. Moreover, making sure that passengers are entertained throughout the flight (e.g. movies, headphones with music) will definitely help them look back at the flight with nothing but enjoyment.

While thinking of the least important variables we have one immediate candidate - WifiNote - it seems like availability and quality of WiFi service does not matter that much to the customers.\

Now, with the help of the ROCurve, we will check the performance of this model.

```{r}
# function to get AUC
fAUC_RF <- function(model, data, ...) {
  y <- all.vars(formula(model))[1]
  pred1 <- as.numeric(predict(model, newdata = data, ...))
  pred <- ROCR::prediction(pred1, data[[y]])
  perf <- performance(pred, "auc")
  AUC <- attr(perf, "y.values")[[1]]
  AUC
}

AUC_RanForest1_train <- fAUC_RF(RanForest1, data = Airlines_binned_train, type = "response")
AUC_RanForest1_test <- fAUC_RF(RanForest1, data = Airlines_binned_test, type = "response")
pred1_RF <- prediction(as.numeric(predict(RanForest1, type = "prob")[ ,2]), Airlines_binned_train$IsSatisfied)

wrapped_fauc_rf <- function(d){fAUC_RF(RanForest1, data = d, type = "response")}
AUC_RF1_toConclusion <- lapply(Airlines_binned_test_splited, wrapped_fauc_rf)

# Visualize the ROC curve:
AUC1_RF <- attr(performance(pred1_RF, "auc"), "y.values")[[1]]
perf1_RF <- performance(pred1_RF, "tpr", "fpr")
ks1_RF <- max(attr(perf1_RF,'y.values')[[1]] - attr(perf1_RF,'x.values')[[1]])
predSc1_RF <- as.numeric(predict(RanForest1, type = "prob")[ ,2])
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predSc1_RF)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predSc1_RF)
```

The model has an AUC of `r AUC1_RF` on the train set and `r AUC_RanForest1_test` on the test set. As we observe a drop of AUC presume that this model shows signs of overfitting to the train set.

We decided to explore some options to improveme the model but did not succeed. Excluding any variables resulted in making accuracy worse, and still not stable on the test set. Adding interactions didn't change accuracy or performance of the model at all. We therefore concluded on the original model as the for Random Forest representative.

We can now approach validation part.

### Validation
We will validate the model using the Monte Carlo Cross Validation. We will draw 100 times a training data-subset containing of 70% of observations, fit the model and then study the AUC on the train and test sets.
```{r}
pctTrain <- 0.7
set.seed(1201984)
nRuns = 20
```

```{r fig.cap='The results of the cross validation for the random forest.\\label{fig:MCxvalrf1}'}
allAUCs_RF_path <- "./random_forest/rf_allAUCs_RF.rds"
AUCs_test_RF_path <- "./random_forest/rf_test_AUCs_RF.rds"

allAUCs_RF <- if(file.exists(allAUCs_RF_path)){
                    readRDS(allAUCs_RF_path)
              }else{
                    cv_mc_RF <- crossv_mc(Airlines_binned_train, n = nRuns, test = 1 - pctTrain)
                    mods_RF<-map(cv_mc_RF$train,~RanForest1)
                    AUCs_train_RF <- numeric(0)
                    AUCs_test_RF <- numeric(0)
                    for(k in 1:nRuns) {
                        AUCs_test_RF[k] <- fAUC_RF(mods_RF[[k]], as.data.frame(cv_mc_RF$test[[k]]))
                        AUCs_train_RF[k] <- fAUC_RF(mods_RF[[k]], as.data.frame(cv_mc_RF$train[[k]]))
                    }
                    saveRDS(AUCs_test_RF, AUCs_test_RF_path)
                    
                    allAUCs_RF <- rbind(
                      tibble(model = "train data", AUC = AUCs_train_RF),
                      tibble(model = "test data", AUC = AUCs_test_RF)) %T>%
                      saveRDS(allAUCs_RF_path)
              }
AUCs_test_RF <- readRDS(AUCs_test_RF_path)
p1 <- ggplot(allAUCs_RF, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5)
p2 <- ggplot(allAUCs_RF, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
```

Observed values for the AUC of the test data have the median equal to `r median(AUCs_test_RF)`, the average equal to `r mean(AUCs_test_RF)` and standard deviation equal to `r sd(AUCs_test_RF)`. Even though from the plots it seems like we achieve a larger variance of AUCs on test set, it is nevertheless still a small number. This means our model has worse, yet stable performance on the test set. We are content with the results, as we have AUCs of training and test sets keep in line with each other. The model may be a bit over-fitted to train set, but manages to provide a consistent performance on the test set.

## Logistic Regression

### Fitting and performance
The next model that we will study is a logistic regression. We start from the most basic model which takes all the variables. 
```{r}
# Define the formula:
frm <- IsSatisfied ~.
# Fit the model:
logreg1 <- glm(formula = frm, data = Airlines_binned_train, family = "binomial")
# Investigate the model:
summary(logreg1)

```
We notice that according to this model:

* business class passengers have a much higher satisfaction level than eco plus class passengers, who in their turn have a higher satisfaction level than eco class ones,
* satisfaction level is higher for females and loyal customers,
* passengers at the age of 40s and 50s have a higher satisfaction level than the others,
* satisfaction level is lower for long and medium flight distances than for the short ones,
* the higher entertainment, seat, eBooking, eSupport, service, leg room, clean, baggage and check-in notes, the higher the satisfaction level.

What we find unintuitive is that taking into account wifi, food and eBoarding the satisfaction level is higher for medium notes than for the high ones. This may be a consequence of variable interactions: wifi and food are usually available during longer flights. However, the longer the flight, the more people are tired, uncomfortable etc. Nevertheless, we don't have enough background information to make such conclusions. We decide to leave these variables in our model for now and check its performance, but later we will try do improve it.
```{r}
library(ROCR)
# function to get AUC
fAUC <- function(model, data, ...) {
y <- all.vars(formula(model))[1]
pred1 <- predict(model, newdata = data, ...)
pred <- ROCR::prediction(pred1, data[[y]])
perf <- performance(pred, "auc")
AUC <- attr(perf, "y.values")[[1]]
AUC
}
AUC_logreg1_train <- fAUC(logreg1, data = Airlines_binned_train, type = "response")
AUC_logreg1_test <- fAUC(logreg1, data = Airlines_binned_test, type = "response")
pred1 <- prediction(predict(logreg1, type = "response"), Airlines_binned_train$IsSatisfied)

wrapped_fauc_lr <- function(d){fAUC(logreg1, data = d, type = "response")}
AUC_LR1_toConclusion <- lapply(Airlines_binned_test_splited, wrapped_fauc_lr)

# Visualize the ROC curve:
AUC1 <- attr(performance(pred1, "auc"), "y.values")[[1]]
perf1 <- performance(pred1, "tpr", "fpr")
ks1 <- max(attr(perf1,'y.values')[[1]] - attr(perf1,'x.values')[[1]])
predScores1 <- predict(logreg1, type = "response")
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores1)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores1)
```

The AUC on the training data is `r AUC1` and on the testing data `r AUC_logreg1_test`. The difference is marginal. KS plot and the ROC curve also look great. This model is already a good challenger. 

However, we decided to add interactions between variables as some of them seem natural. After many attempts we opt for including the interactions between gender&loyalty, gender&class, gender&age and class&flight distance. We tried also for example class&food or class&service, but they didn't improve our model. 

What is more, we decided to not include WifiNote as after adding interactions it was the least significant feature and the relationship was counterintuitive. Thus we get the final logistic regression model, which is summarized below.
```{r}
frm2 <- IsSatisfied ~.+IsFemale*IsLoyal+IsFemale*Age+IsFemale*Class+Class*FlightDistance-WifiNote
# Fit the model:
logreg2 <- glm(formula = frm2, data = Airlines_binned_train, family = "binomial")
# Investigate the model:
summary(logreg2)
```

We notice that according to our model:

* business class passengers have a much higher satisfaction level than eco plus class passengers, who in their turn have a higher satisfaction level than eco class ones,
* satisfaction level is higher for females and loyal customers,
* passengers at the age of 40s and 50s have a higher satisfaction level than the others,
* the higher entertainment, seat, eBooking, eSupport, service, leg room, clean, baggage and check-in notes, the higher the satisfaction level,
* generally, the longer the flight distance, the less satisfied people are. However, this is not the case in business cass, where people are more satisfied on longer distances,
* females travelling business and eco plus class are less satisfied than the ones from eco class and females over 60 are more satisfied than the younger ones.


Now we will study the performance of the model.
```{r rocr, fig.cap=c("The ROC (receiver operating curve) for our model", "The lift of the model (bottom): the cumulative percentage of responders (ones) captured by the model")}
AUC_logreg2_train <- fAUC(logreg2, data = Airlines_binned_train, type = "response")
AUC_logreg2_test <- fAUC(logreg2, data = Airlines_binned_test, type = "response")
pred2 <- prediction(predict(logreg2, type = "response"), Airlines_binned_train$IsSatisfied)

wrapped_fauc_lr2 <- function(d){fAUC(logreg2, data = d, type = "response")}
AUC_LR2_toConclusion <- lapply(Airlines_binned_test_splited, wrapped_fauc_lr2)

# Visualize the ROC curve:
AUC2 <- attr(performance(pred2, "auc"), "y.values")[[1]]
perf2 <- performance(pred2, "tpr", "fpr")
ks2 <- max(attr(perf2,'y.values')[[1]] - attr(perf2,'x.values')[[1]])
predScores2 <- predict(logreg2, type = "response")
ks_plot(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores2)
cat('\n\n')
plotROC(actuals = Airlines_binned_train$IsSatisfied, predictedScores = predScores2)
```

This logistic regression model has an AUC of `r AUC2` on the training data and `r AUC_logreg2_test`on the testing data. The difference is again marginal. The KS is `r ks2`. Moreover, we can see (on the ROC curve graph) that our classifier is not far from being perfect. Therefore, we will move forward to the validation part. 

### Validation
To validate the model we will use the cross validation method and opt for the Monte Carlo Cross Validation.
```{r}
pctTrain <- 0.7
set.seed(18901229)
nRuns = 20
```
We use the Monte Carlo Cross Validation with a test data-set that spans 30% of our observations and 70% in the training data-set. We will draw 100 times a training data-set of 0.7 and study the AUC on the testing data-set.
```{r fig.cap='The histogram for the AUC of the ramdomised data-sets with the Monte Carlo cross vailidation for the first logistic regression model.\\label{fig:MCxval1}'}
cv_mc <- crossv_mc(Airlines_binned_train, n = nRuns, test = 1 - pctTrain)
mods <- map(cv_mc$train, ~ glm(frm2, data = ., family = "binomial"))
#AUCs <- map2_dbl(mods, cv_mc$test, fAUC)
RMSE <- map2_dbl(mods, cv_mc$test, rmse)
AUCs_train <- numeric(0)
AUCs_test <- numeric(0)
for(k in 1:nRuns) {
AUCs_test[k] <- fAUC(mods[[k]], as.data.frame(cv_mc$test[[k]]))
AUCs_train[k] <- fAUC(mods[[k]], as.data.frame(cv_mc$train[[k]]))
}
allAUCs_lr2 <- rbind(tibble(model = "train data", AUC = AUCs_train),
tibble(model = "test data", AUC = AUCs_test))
p1 <- ggplot(allAUCs_lr2, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5) + xlim(0.9520, 0.9620)
p2 <- ggplot(allAUCs_lr2, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
#median(AUCs_test)
#mean(AUCs_test)
#sd(AUCs_test)

```
We are very satisfied with the performance of our model. The median of the observed values for the AUC of the test data is `r median(AUCs_test)`, the average is `r mean(AUCs_test)` with a standard deviation of `r sd(AUCs_test)`. We see that dispersion of AUCs during cross-validation is minuscule, and similar between cross-validated test and train sub-sets. We find these results more than satisfactory.

## Neural Network model

Although we can't use Neural Networks to explain customer satisfaction, we also made an attempt to solve the problem with a neural network to understand the upper bound for performance that is achievable on the given problem.

```{r}
nn_model_path <- "./neural_net/nn_model.h5"
nn_history_path <- "./neural_net/nn_learning_history.rds"
nn_cv_data_path <- "./neural_net/nn_cv.rds"
```

```{r}
train_matrix <- AirlinesEncoded_binned_train %>%
                mutate_all(as.character) %>%
                mutate_all(as.numeric) %>%
                as.matrix

test_matrix <- AirlinesEncoded_binned_test %>%
                mutate_all(as.character) %>%
                mutate_all(as.numeric) %>%
                as.matrix
```

```{r, message = FALSE, warning = FALSE}
train_input_x <- train_matrix[, colnames(train_matrix) != "IsSatisfied"]
train_input_y <- to_categorical(train_matrix[, colnames(train_matrix) == "IsSatisfied"], 2)

test_input_x <- test_matrix[, colnames(test_matrix) != "IsSatisfied"]
test_input_y <- to_categorical(test_matrix[, colnames(test_matrix) == "IsSatisfied"], 2)
```

The network consists of the following layers:

* input with 34 nodes,
* 3 hidden ones with 24, 16, 8 nodes respectively,
* output with 2 nodes.

In each layer we have used ReLU (Rectified Linear Unit) activation function, and to regularize the model we applied a dropout of 0.3.

```{r, echo = FALSE}
create_nn_model <- function() {
  nn_model <- keras_model_sequential() %>%
    layer_dense(units = 34, activation = 'relu',
                input_shape = dim(train_input_x)[2]) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 24, activation = 'relu') %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 16, activation = 'relu') %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 8, activation = 'relu') %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 2, activation = 'softmax')
  nn_model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(lr = 0.01),
    metrics = c('accuracy')
  )
  nn_model
}
nn_model <- create_nn_model()
summary(nn_model)
```

### Fitting and performance

We gave the model 25 epochs to fit to the binned and encoded dataset, with a batch size of 64. During fitting, a sample of 0.1 was used a as a control/validation data to make sure we're not overfitting the model.

```{r}
# NN training or restoring
fit_nn_model <- function(model, data_x, data_y) {
  history <- model %>%
      fit(data_x, data_y, epochs = 25, batch_size = 64,
          validation_split = 0.1)
  history
}

history <- if(file.exists(nn_history_path) & file.exists(nn_model_path)){
                nn_model <- load_model_hdf5(nn_model_path)
                readRDS(nn_history_path)
          }else{
              h <- fit_nn_model(nn_model, train_input_x, train_input_y) %T>%
                saveRDS(nn_history_path)
              nn_model %>% save_model_hdf5(nn_model_path)
              h
            }
```

```{r, include = TRUE}
plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", ylim= c(0, 1), col="blue", type="l")
lines(history$metrics$val_loss, col="red")
legend("topright", c("train","validation"), col=c("blue", "red"), lty=c(1,1))
```

```{r, include = TRUE}
plot(history$metrics$acc, main="Model Accuracy", xlab = "epoch", ylab="accuracy", ylim= c(0, 1), col="blue", type="l")
lines(history$metrics$val_acc, col="red")
legend("bottomright", c("train","validation"), col=c("blue", "red"), lty=c(1,1))
```

Surprisingly, the neural network has achieved most of its performance after only one epoch. This can be seen in the charts above, and explainable by high level of data separability - which was the reason why even the baseline model achieved high performance.

```{r}
fAUC_NN <- function(model, data_x, data_y) {
  pred1 <- predict(model, data_x)
  pred <- ROCR::prediction(pred1, data_y)
  perf <- performance(pred, "auc")
  AUC <- attr(perf, "y.values")[[1]]
  AUC
}
AUC_nn_train <- fAUC_NN(nn_model, train_input_x, train_input_y)
AUC_nn_test <- fAUC_NN(nn_model, test_input_x, test_input_y)
```

```{r}
#to conclusion paragraph
AUC_NN_toConclusion <- numeric(0)
idx = 1
for(i in AirlinesEncoded_binned_test_splited){
  m <- i %>%
       mutate_all(as.character) %>%
       mutate_all(as.numeric) %>%
       as.matrix
  input_x <- m[, colnames(m) != "IsSatisfied"]
  input_y <- to_categorical(m[, colnames(m) == "IsSatisfied"], 2)
  AUC_NN_toConclusion[idx] <- fAUC_NN(nn_model, input_x, input_y)
  idx = idx + 1
}
```

```{r}
#AUC_NN_toConclusion
```


```{r}
pred1 <- prediction(predict(nn_model, train_input_x), train_input_y)
AUC1 <- attr(performance(pred1, "auc"), "y.values")[[1]]
perf1 <- performance(pred1, "tpr", "fpr")
ks1 <- max(attr(perf1,'y.values')[[1]] - attr(perf1,'x.values')[[1]])
```

```{r}
# Visualize the ROC curve:
predScores1 <- predict(nn_model, train_input_x)
ks_plot(actuals = as.numeric(train_input_y), predictedScores = as.numeric(predScores1))
plotROC(actuals = train_input_y, predictedScores = predScores1)
```

### Validation
To keep consistency with other models, we have also used Monte Carlo Cross Validation for the Neural Network despite the high computational cost.

```{r}
nn_cv_data_path <- "./neural_net/cv.rds"

allAUCs_NN<-if(file.exists(nn_cv_data_path)){
              readRDS(nn_cv_data_path)
            }else{
              pctTrain <- 0.7
              set.seed(67789)
              nRuns = 20
                
              AUCs_train_NN <- numeric(0)
              AUCs_test_NN <- numeric(0)
              N <- nrow(train_matrix)
                for(k in 1:nRuns) {
                  train_index <- sample(1:N, pctTrain * N)
                  test_index  <- setdiff(1:N, train_index)
                
                  k_model <- create_nn_model()
                  fit_nn_model(k_model,
                          train_matrix[train_index, colnames(train_matrix) != "IsSatisfied"],
                          to_categorical(train_matrix[train_index, colnames(train_matrix) == "IsSatisfied"]))
                
                  AUCs_train_NN[k] <- fAUC_NN(k_model,
                          train_matrix[train_index, colnames(train_matrix) != "IsSatisfied"],
                          to_categorical(train_matrix[train_index, colnames(train_matrix) == "IsSatisfied"]))
                  AUCs_test_NN[k] <- fAUC_NN(k_model,
                          train_matrix[test_index, colnames(train_matrix) != "IsSatisfied"],
                          to_categorical(train_matrix[test_index, colnames(train_matrix) == "IsSatisfied"]))
                  }

              allAUCs_NN <- rbind(
                  tibble(model = "train data", AUC = AUCs_train_NN),
                  tibble(model = "test data", AUC = AUCs_test_NN)) %T>%
                saveRDS(nn_cv_data_path)}

p1 <- ggplot(allAUCs_NN, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5) + xlim(0.975, 0.985)
p2 <- ggplot(allAUCs_NN, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
```
```{r}
aucs_train_nn <- allAUCs_NN %>% filter(model == "train data") %>% select(AUC) %>% as.matrix()
aucs_test_nn <- allAUCs_NN %>% filter(model == "test data") %>% select(AUC) %>% as.matrix()
```



Validation showed that the model is highly effective.

The AUC for the neural network model on training data has:

* the median equal to `r median(aucs_train_nn)` for train data and `r median(aucs_test_nn)` for test data,
* the average equal to `r mean(aucs_train_nn)` for train data and `r mean(aucs_test_nn)` for test data,
* the standard deviation equal to `r sd(aucs_train_nn)` for train data and `r sd(aucs_test_nn)` for test data.

Moreover, the density plots behave as one would expect - they have a distribution of similar dispersion, but the test data is slightly displaced to the left. Nonetheless, this displacement is acceptable and explainable since in the end model is expected to perform better on the train dataset.

# Conclusion

```{r concl, fig.cap='The kernel density for the observed areas under the curve (top) and the cumulative probability density functions (bottom)for the challenger models. All AUCs shown are for the test data only.'}

allAUCs <- rbind( tibble(model = "logistic reg1", AUC = AUC_LR1_toConclusion %>% unlist()),
                  tibble(model = "logistic reg2", AUC = AUC_LR2_toConclusion %>% unlist()),
                  tibble(model = "random forest", AUC = AUC_RF1_toConclusion %>% unlist()),
                  tibble(model = "neural network", AUC = AUC_NN_toConclusion %>% unlist()))

p1 <- ggplot(allAUCs, aes(AUC, fill = model, colour = model)) + geom_density(alpha=0.5)
p2 <- ggplot(allAUCs, aes(AUC, fill = model, colour = model)) + stat_ecdf()
grid.arrange(p1, p2, ncol = 1)
```

In the plots, we showed the accuracy of models with the data they had not seen before.
As we can see, all challangers models performed well.
The neural network model is the most accurate, but we recommend logistic regression 2 for production - since it is transparent, and explainable so its operation can provide insight about customer preferences for the Airlines Company. 

Moreover, the effects achieved by the logistic regression model are of very high performance metrics.

In the following table we summarise these results:

Model   |   Mean AUC on Test Data | Mean AUC on Training Data
--------+----------+----------------+--------------------------+----------------
logistic regression 1     |  `r mean(AUC_LR1_toConclusion %>% unlist())`      | `r mean(AUC_logreg1_train)`
logistic regression 2     |  `r mean(AUC_LR2_toConclusion %>% unlist())` | `r mean(allAUCs_lr2$AUC)`
random forest     |  `r mean(AUC_RF1_toConclusion %>% unlist())` | `r mean(allAUCs_RF$AUC)`
neural network    |  `r mean(AUC_NN_toConclusion %>% unlist())`        | `r mean(allAUCs_NN$AUC)`


# Bibliography
De Brouwer, Philippe J.S. 2020. The Big r-Book: From Data Science to Learning Machines and Big Data. New York: John Wiley & Sons, Ltd.