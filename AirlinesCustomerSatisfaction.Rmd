---
title: |
  | \LARGE \textsf The Selection of a Model for Airlines Customer Satisfaction
  | \vspace{4ex}
author: "Joanna Krężel, Anna Matysek, Piotr Mikler, Adam Szczerba"
date: "`r format(Sys.time(), '%d %B, %Y')`"
abstract: "The project aims to analyze the data about satisfaction of Investico Airlines passengers. Having customer-granular observations about the cruise and a reported satisfaction level for particular aspects of the flight we try to fit models which predict whether they are satisfied with the service or not. The binary classification models used during the project are {} {} {}, out of which our recommendation is {} based on {}. This document describes the process we undertook and presents the results of data preprocessing, model selection and model validation."
documentclass: article 
classoption:
  - 12pt
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
header-includes:
    \usepackage{titling}
    \pretitle{\begin{center}
    \vspace{-7ex}
    \includegraphics[height=40mm]{img/logo3AGH.png}\\
    \vspace{5ex}
    {\large \bf \textsf{AGH UNIVERITY OF SCIENCE AND TECHNOLOGY}}\\
    {\large \bf \textsf{FACULTY OF APPLIED MATHEMATICS}}\\
    \vspace{10ex}
    }
---

<!--
Alternative, use the mgrwms template:
https://code.google.com/archive/p/mgrwms/downloads
http://zasoby.open.agh.edu.pl/~12sjkaminski/indexba7d.html?q=pl/content/klasy-agh
-->


\newpage

\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(forcats)
library(InformationValue)
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
library(ROCR)
library(modelr)
library(purrr)
library(httr)
library(caret)

# SETUP
# Some config variables
bDownloadFromKaggle <- FALSE  # fetch data from Kaggle when compiling .Rmd
apiCredentials <- list(username = "piotrmikler",
                       key = "8453a8073400c47671d87e6a1b0fe7da")  # leave as-is
setwd("D:/Classes/Mgr/Semestr_3/Quantitative_Analysis_for_Managerial_Decisions/Airlines") # change for your main directory
```

\newpage

# Introduction

_Tbd..._

# The Data

```{r}
fetch_from_kaggle <- function(sOutputPath, apiCredentials){
  #' Download the project dataset from Kaggle API
  #' 
  #' @param sOutputPath path-like string, location where to save data
  #'
  #' @param apiCredentials Kaggle API credentials, list of username and key

  sZipFile <- file.path(sOutputPath, 'Dataset.zip')
  apiResponse <- httr::GET("https://www.kaggle.com/api/v1/datasets/download/sjleshrac/airlines-customer-satisfaction",
                           httr::authenticate(apiCredentials$username, 
                                              apiCredentials$key, 
                                              type="basic"))
  # Download and unzip
  download.file(apiResponse$url, 
                destfile = sZipFile, 
                mode="wb")
  unzip(zipfile = sZipFile, 
        file = 'Invistico_Airline.csv',
        exdir = file.path(sOutputPath, "Data"))
  unlink(sZipFile)
}

if (bDownloadFromKaggle){
  fetch_from_kaggle(sOutputPath = getwd(),
                    apiCredentials = apiCredentials)
}

sFileName <- file.path(getwd(), '/Data/Invistico_Airline.csv')

Airlines <- read_csv(sFileName,
                     col_types = "fffdffdffffffffffffffdd")
```

The data is downloaded from [www.kaggle.com](https://www.kaggle.com/sjleshrac/airlines-customer-satisfaction?fbclid=IwAR1azFXqBkqKfah7ZTJ16CcR6S-zYpdQMJ7z7hlXcrEtzcJgh6nlN_4Bu8Y) and delivered by an airline organization. The
dataset consists of the details of customers who have already flown with them. The feedback of
the customers on various context and their flight data has been consolidated. The main purpose
of this dataset is to predict whether a future customer would be satisfied with their service given
the details of the other parameters values. Also the airlines need to know on which aspect of the
services offered by them have to be emphasized more to generate more satisfied customers.
The data consists of 129880 rows and 23 columns.

Below we list all column names with explanations of the variables' meaning.
For categorical variables in the dataset describing satisfaction level, $0$ means _Not Applicable_.

Feature | Description | Values
--------|-------------|-------
Satisfaction | Airline satisfaction level | satisfied/dissatisfied
Gender | Gender of the passenger | male/female
Customer type | The customer type | loyal / disloyal customer
Age | The age of a passenger | [7; 85] years
Type of travel | Purpose of the flight | personal / business travel
Class | Travel class in the plane | business / eco / eco plus
Flight distance | The flight distance of the journey | [50; 6951] miles
Seat comfort | Satisfaction level of seat comfort | {0-5}
Departure/arrival | Satisfaction level of departure/arrival time | {0-5}
Food and drink | Satisfaction level of food and drink | {0-5}
Gate location | Satisfaction level of gate location | {0-5}
Inflight WiFi service | Satisfaction level of the inflight wifi service | {0-5}
Inflight entertainment | Satisfaction level of inflight entertainment | {0-5}
Online support | Satisfaction level of online support | {0-5}
Ease of online booking | Satisfaction level of online booking | {0-5}
On-board services | Satisfaction level of on-board service | {0-5}
Leg room service | Satisfaction level of leg room service | {0-5}
Baggage handling | Satisfaction level of baggage handling | {0-5}
Checkin service |Satisfaction level of check-in service | {0-5}
Cleanliness | Satisfaction level of cleanliness | {0-5}
Online boarding | Satisfaction level of online boarding | {0-5}
Departure delay in minutes | Delay upon departure | [0; 1592] minutes
Arrival delay in minutes | Delay upon arrival | [0; 1584] minutes

## Data Preprocessing

As both academia and business point out, the data-related operations typically constitute about $80\%$ of the whole effort of a modeling pipeline. The performance of any model is heavily driven by the quality of it's inputs. It can be easily proven in a simple trial by combat that even a suboptimal model running on high quality data can oftentimes bring a sophisticated one with poor inputs to it's knees. For that reason it is of utmost importance to pay extra care and attention to the data which is fed to the decision making models.

As the first step in our modeling pipeline we are going to look at the dataset to gain insight about it's statistics and information it conveys. We'll refactor the feature names to something more manageable and represent accordingly different data types present in the dataset. We will perform quality checks on the data, such as outlier detection and treatment of not-available values (*NA*s). Lastly in this pre-modeling step we will look at the actual *information* carried by particular features to remove variables which are unlikely to have significant added value. The following section will be divided into parts corresponding to the data type of the features.

### Categorical features

The dataset contains some binary categorical information such as *Male/Female*, *Loyal/Disloyal Customer*, etc. We are going to employ binary encoding for those features, that is: map values to $1$ or $0$ and rename the factors to  `IsSatisfied`, `IsFemale`, `IsLoyal` for easier interpretation.

```{r}
sCategorialColnames <- c('satisfaction', 'Gender', 'Customer Type', 'Type of Travel')
Airlines <-
  Airlines %>%
  mutate(IsSatisfied = as_factor(ifelse(satisfaction == "satisfied", 1, 0)),
         IsFemale = as_factor(ifelse(Gender == "Female", 1, 0)),
         IsLoyal = as_factor(ifelse(`Customer Type` == "Loyal Customer", 1, 0)),
         IsPersonalTravel = as_factor(ifelse(`Type of Travel`== "Personal Travel",1, 0))) %>%
  select(-all_of(sCategorialColnames)) %>%
  rename("FlightDistance" = "Flight Distance",
         "SeatNote" = "Seat comfort", 
         "ScheduleNote" = "Departure/Arrival time convenient",
         "FoodNote" = "Food and drink",
         "GateNote" = "Gate location",
         "WifiNote" = "Inflight wifi service",
         "EntertainmentNote" = "Inflight entertainment",
         "eSupportNote" = "Online support",
         "eBookingNote" = "Ease of Online booking",
         "ServiceNote" = "On-board service",
         "LegRoomNote" = "Leg room service",
         "BaggageNote" = "Baggage handling",
         "CheckInNote" = "Checkin service",
         "CleanNote" = "Cleanliness",
         "eBoardingNote" = "Online boarding",
         "DepartureDelay" ="Departure Delay in Minutes",
         "ArrivalDelay" = "Arrival Delay in Minutes")
```

### Ordinal features

The main challenge of the data preparation in this dataset is the treatment of ordinal features.
Take for example the `SeatNote` feature which is a customer note describing their satisfaction level with the seating arrangement. One could ask himself the following questions:

* What did the passenger have in mind? Satisfaction with seat location? Seat comfort? Possibility of choosing the seat?
* Does $`SeatNote` = 3$ imply a negative attitude towards a service? Or it's a moderate 'OK'?
* Is the satisfaction *"difference"* between notes $3$ and $2$ the same as between notes $5$ and $4$?
* Is a note $`SeatNote` = 5$ given $`Class` = `Eco`$ the same as $`SeatNote` = 5$ given $`Class` = `Business`$?

The point is valid for any note-type variable in the dataset. The issue boils down to the problem that there is **no universal "unit" of satisfaction**. It is just as non-trivial to measure it as to predict it - simply because everyone perceives it in a subjective way. Our problem has an additional layer of complexity since we don't have information how precisely the survey questions have been described to the customers - so even if we *did* have some carefully designed satisfaction unit, we cannot be sure if all respondents referred to the same aspects of service when filling out the survey.

Before discussing this further let's take a short detour to the options we have when dealing with ordinal variables for Machine Learning. Two most common approaches emerge: **Dummy encoding** and **Ordinal encoding** - both are valid, depending on what we're trying to achieve. 

We could use ordinal encoding and assign numbers to each vote. This is pretty much what we already have in our "note" features. We could encode `Class` this way and assign a mapping like: {'Eco': 1, 'EcoPlus': 2, 'Business': 3}. This type of representation ensures the quality of the service is properly represented in the numeric data, but the question is whether this translates the same to the overall satisfaction? Yes, the business class is clearly more comfortable to travel in, but the *expectations* (the baseline) of business-class passengers will also be quite higher than the expectations of say, passengers in the economic class. This may result in a counterintuitive drop in the satisfaction level, simply because the sub-populations across business classes will perceive the service differently. 

The second possibility we have for encoding ordinal variables is the *dummy encoding* which will split the feature `Class` into features: `Class.Eco`, `Class.EcoPlus` and `Class.Business` assigning ones and zeros in appropriate places. One of those features will be dropped to avoid perfect linear relationship (otherwise the sum of the new features would always be $1$), but we'll not lose information. We only need $n-1$ features to encode full information about a factor with $n$ possible levels.

We chose to employ dummy encoding to encode `Class` - to avoid making assumptions about baseline satisfaction criteria across different passenger classes. For `Note` features however this problem is non-existent, since a higher note should correspond to higher satisfaction for any rational passenger. Here to avoid inflating the dataset with additional $4*14 - 14 = 42$ sparse binary columns we will stick to the original ordinal encoding. This choice nonetheless should be revisited and controlled once we reach the stage of model choice and model fitting.

The resulting, encoded dataframe looks the following way:
```{r}
Airlines <- Airlines %>% 
  mutate(Class = as_factor(str_replace(Class, " ", "")))

AirlinesEncoded <- 
  dummyVars(~Class, data = Airlines,
            fullRank = TRUE, drop2nd = TRUE) %>%
  predict(newdata = Airlines) %>%
  as_tibble() %>%
  bind_cols(Airlines) %>%
  select(-Class) %>%
  mutate_at(vars(matches("Class")), as_factor)

AirlinesEncoded %>% glimpse()
```  


```{r}
AirlinesEncoded %>%
  select(starts_with("Is")) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>% 
  mutate(value = ifelse(value==1, "YES", "NO"),
         IsSatisfied = ifelse(IsSatisfied==1, "YES", "NO")) %>%
  ggplot(aes(x = IsSatisfied, fill=IsSatisfied)) + 
  geom_bar() + facet_grid(value~Feature) +
  theme_minimal() + ylab("# Satisfied/dissatisfied") 

AirlinesEncoded %>%
  select(c(ends_with("Note"), ends_with('Satisfied'))) %>%
  mutate_all(as.numeric) %>%
  pivot_longer(cols=!ends_with('Satisfied'), names_to = 'Feature') %>% 
  mutate(IsSatisfied = as_factor(ifelse(IsSatisfied==1, "YES", "NO")),
         Feature = as_factor(Feature),
         value = as_factor(value)) %>%
  ggplot(aes(x = value, fill=IsSatisfied)) + geom_bar()# + 
  #facet_grid(Feature~) + theme_minimal() + ylab("# Satisfied/dissatisfied")
```

### Exploratory Data Analysis

**Describe the ideas of this section. To be done, not urgent...**

Data summary
```{r}
Airlines %>% summary()
```

### NA treatment

In the dataset we only have _NAs_ for the `ArrivalDelay` feature. As there is an option to assign $0$ to the delay, and there is non-zero data about `DepartureDelay` we believe these _NAs_ are a genuine data loss. We stand before three possible choices:

* impute the missing values
* drop them from the dataset
* discard the feature from the dataset

Technically, we could do all of them. Looking however at the scatterplot of `ArrivalDelay` vs `DepartureDelay` we see these variables are extremely higly correlated, especially in their positive tails. This seems in line with intuition as the airplane departure delay should translate to delay in it's arrival roughly linearly. 
```{r}
c <- cor(x = drop_na(Airlines)$DepartureDelay, y = drop_na(Airlines)$ArrivalDelay)
Airlines %>% drop_na() %>% 
  ggplot(aes(x = DepartureDelay, y = ArrivalDelay)) +
  geom_point() + geom_smooth() +
  ggtitle(paste0("Collinearity of plane Departure and Arrival delays (pearson corr. coeff.:", round(c, 4), ")"))
```

Therefore we could _technically_ easily impute the values by regressing `ArrivalDelay` on `DepartureDelay` - however given the high correlation of those variables (`r round(c, 4)` pearson correlation coefficient) one of them is bound to be dropped during multicollinearity analysis. For this reason we are not going to bother imputing the missing values, but will simply drop `ArrivalDelay` from the features.

### Feature Selection

The more is not always the better. Every model has a certain computational complexity that increases with the number of additional explanatory variables. The feature selection in a pre-modeling environment serves identifying groups of variables which carry repeated or very similar informational value. *Filtering feature selection methods* allow one to discard redundant features in a model independent way. By reducing the number of variables they simplify the model and increase it's interpretability. It is also a step which tackles multicollinearity (high linear codependency of explanatory variables) which kills stability and predictive power of some models.

## Categorical Variables

### The information Value

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


## The Continous Variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### Decide which Continuous Variable to Use

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Data Binning

### The Categorical Variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

### The Continuous variables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# The Logisic Regresssion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# The performance of the Model

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Validation of the Model

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


## Monte Carlo Cross Validation


Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# The Challenger Models

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Neural Network

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Another logistic regression: logistic 2

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


# Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

# Bibliography